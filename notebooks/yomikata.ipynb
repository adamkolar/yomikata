{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c78829-e0f9-446d-b7e1-ea56570185ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Yomikata: Disambiguating Japanese Heteronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15e82b-0f6d-4975-9a67-b6a51f2392b7",
   "metadata": {},
   "source": [
    "A step by step guide to training Yomikata's word disambiguation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb6478-ff14-49d8-a4e7-2421059efd6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word pronunciation lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3db016-1d49-4167-9d74-9832b70965cd",
   "metadata": {},
   "source": [
    "To clean the datasets we use it is useful to have a list of Japanese words and their pronunciations. \n",
    "\n",
    "We do that by parsing the unidic and sudachi dictionaries. Note these scripts are slow -- but run one time only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca4531-d241-4bee-b30a-531ac1ba56a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.unidic import unidic_data\n",
    "\n",
    "unidic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3bc77c-1d3c-4ffd-b262-195a276d8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.sudachi import sudachi_data\n",
    "\n",
    "sudachi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592781a9-95ea-4be7-a71d-5ea0df0e9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.kanjidic import kanjidic_data\n",
    "\n",
    "kanjidic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fba305-77b6-4ce9-a7af-4d5eac389d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.pronunciations import pronunciation_data\n",
    "\n",
    "pronunciation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b059a9-a88f-4fe2-ac27-6c93f4a30387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from yomikata.config import config\n",
    "\n",
    "df = pd.read_csv(Path(config.READING_DATA_DIR, \"all.csv\"))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e8cbc-d036-405c-8963-4923a151ba0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Corpuses of annotated sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9db4c-9c21-4959-aa09-d96846c46163",
   "metadata": {},
   "source": [
    "The model is trained on sentences which already have furigana. We have four data sources which we process here. Note these scripts are slow -- but run one time only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b586b-1a9d-4c17-95ec-fd9e4bc3e964",
   "metadata": {},
   "source": [
    "[Corpus of titles of works in the national diet library](https://github.com/ndl-lab/huriganacorpus-ndlbib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b16cf5-e703-4d64-aa17-57b22218c7b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from yomikata.dataset.ndlbib import ndlbib_data\n",
    "\n",
    "# ndlbib_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58623bf-e5c9-4193-83c4-e15206a2caa3",
   "metadata": {},
   "source": [
    "[Aozora Bunko book corpus](https://github.com/ndl-lab/huriganacorpus-aozora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e2883-046e-4d4c-9c1e-e7ef3486c58d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.aozora import aozora_data\n",
    "\n",
    "aozora_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da6a94-b5a9-4bf2-ab1a-9ef0893bd2f6",
   "metadata": {},
   "source": [
    "[Kyoto University document leads corpus](https://github.com/ku-nlp/KWDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6008d-3429-4272-8610-22cf4eb47495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.kwdlc import kwdlc_data\n",
    "\n",
    "kwdlc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47b68e-7f8c-425d-bdc6-90543a1e15b2",
   "metadata": {},
   "source": [
    "[Search result for our heterophones in the BCCWJ corpus](https://chunagon.ninjal.ac.jp/bccwj-nt/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7de05-40fe-47e7-ab3b-ce64561aaf87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.bccwj import bccwj_data\n",
    "\n",
    "bccwj_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72bd10-b241-4b2a-ac75-c8296d4592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "from yomikata import utils\n",
    "\n",
    "input_files = [\n",
    "    Path(config.SENTENCE_DATA_DIR, \"aozora.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"kwdlc.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"bccwj.csv\"),\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"ndlbib.csv\"),\n",
    "]\n",
    "\n",
    "utils.merge_csvs(input_files, Path(config.SENTENCE_DATA_DIR, \"all.csv\"), n_header=1)\n",
    "logger.info(\"✅ Merged sentence data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbdc34-f293-44fe-bfea-9730d17ab1e7",
   "metadata": {},
   "source": [
    "Filter out duplicate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02935e6c-d2e3-405f-8ccc-01b9f88a8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"all.csv\"))\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sentence'], keep='first')\n",
    "df_no_duplicates.to_csv(Path(config.SENTENCE_DATA_DIR, \"all_filtered.csv\"), index=False)\n",
    "logger.info(\"✅ Filtered out duplicate sentences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a8101-11a9-4ace-b34d-7e0f100fde68",
   "metadata": {},
   "source": [
    "# Spliting furigana in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383890c-b721-4cfb-841a-a7f891058f12",
   "metadata": {},
   "source": [
    "First we generate a dictionary of representations of longer furigana in terms of shorter furigana that appear in the corpus. So for example `{引出/ひきだ}` will be broken down into `{引/ひ}` and `{出/きだ}`. The algorithm attempts to find a set of shorter furigana for which concatenation of surfaces and readings exactly matches the whole or at least the beginning of the longer furigana. It prefers more granular representations (larger amount of shorter furigana) and if two equally granular representations are possible, it picks the one which is composed of furigana with the largest combined frequency in the corpus. It also translates the \"ー\" character into specific hiragana representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bb5f1-afd4-4975-b744-073db6f3c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.breakdown import generate_breakdown_dictionary\n",
    "\n",
    "generate_breakdown_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8301a65-b2d5-40fe-9f8b-d48bca891c6b",
   "metadata": {},
   "source": [
    "Next we use this dictionary to replace all long furigana in the corpus with shorter furigana. By decomposing furigana we get a corpus that allows us to determine readings of kanji surfaces in a more granular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6b860-2148-49eb-bc89-f8f7330af283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.dataset import split\n",
    "from yomikata import utils\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "split_dict = utils.load_dict(Path(config.BREAKDOWN_DATA_DIR, \"translations.json\"))\n",
    "logger.info(\"Starting decomposition process, this may take a while...\")\n",
    "split.decompose_furigana(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"all_filtered.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"all_broken_down.csv\"),\n",
    "    split_dict,\n",
    ")\n",
    "logger.info(\"✅ Decomposed furigana!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c42dde-0741-44cf-97fb-839f23987a4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Making a list of heteronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f391316-42f9-4896-beb9-d731ddb0f952",
   "metadata": {},
   "source": [
    "We use the list from [Sato et al 2022](https://aclanthology.org/2022.lrec-1.770.pdf) as a start. To these we add a list of additional heteronyms picked from the corpus by the frequency of mistakes MeCab tokenizer makes in predicting their readings and arrive at the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5b290-2e1b-409f-a6e2-27d51f07de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heteronyms = \"国立|仮名|遺言|口腔|一途|最中|一行|一夜|下野|花弁|山陰|上下|世論|牧場|一味|施行|施工|転生|清浄|追従|墓石|漢書|作法|黒子|競売|開眼|求道|施業|借家|風車|背筋|逆手|生花|一寸|一分|一文|気骨|細目|船底|相乗|梅雨|風穴|夜話|野兎|冷水|翡翠|十八番|石綿|公文|読本|古本\"\n",
    "heteronyms = \"年中|気味|束|影響|夏|父|理|宝塚|我|私|浩|置|彼|手|右|易|柱|次|是|白髪|文字|博士|造作|相|以|割|引|康|博|告|三重|割当|太鼓|期|近平|眼|許|弥|素|坂|安|綴|血|顎|献|手作|江|畑|七兵衛|野|壁|豊|弘法|生花|顎骨|織|梅|北|弘|対|宴|沢|法衣|突|粗|奇怪|心血|野兎|飛騨|塵|報|身体|島津|船|思|幸|先刻|俺|根|牛|新|力|稲|漢書|染|緒|目|春|与|粒|草子|礼拝|黄|舞鶴|電灯|娘|所|道標|掌|白|都|貝塚|岩|博文|缶詰|造|交換|七十|聞|蜂|枕|友|子|鋼|花|続|化|補綴|深|衣|手術|忍|胎|歯|高山|神|吉|馬鈴薯|土器|荷役|昨夜|止|延|細|受|鼻腔|妾|性骨|如何|兵法|脱水|山|箱|帖|雨|共|不足|風呂|陸奥|興|親|上野|廻|果|学|上手|鼻|針|両眼|藍|法師|府|出|共存|島|川|変換|河|鳥|尼|一夜|版|清浄|位|日暮|九十|風土|訴|今|雨水|白血|疱瘡|地|速|黒|文|群|竹|彩|直|台|橋|右衛門|蛙|処|例|宿|跡|涙|法華|毅|先|奥|星|追従|城|卵|鉄|華|太夫|掛|柳|両側|初|暦|不|躯|心|御|尾|格子|泊|今昔|潮|柳田|腔|市場|夜中|下|裕|塩|何分|合戦|建|河岸|身|浅|家|湖|夫|天|玉手|向|薄|郡|環|話|生命|板|笑|懐|藻|湯|車|瓦|考|道|函|老子|細々|一月|一日|郷|道程|細工|文科|此|脆|勢|園|含|眼鏡|守|妖|輪|婦|影|乳房|洋|艶|終|作|泰|犬|光|東|芽腫|見物|端|脂|鉤|三|刻|仁|色|船底|外|咽喉|下顎|特集|田|構|嫌気|顔|定|長|学校|並|肝|良|取|住居|舟|林|相撲|進|六|飛沫|見|同人|落|綴方|夜|負|法|繰|刃|片|数|居|己|一杯|明|伴|富|気質|最中|信夫|満|於|気|市|戸|旬|刑事|能登|一声|殺|折|五十|敏|一寸|名|公文|隆|判例|焼|故郷|青|立|祭|綱|小屋|河口|南|美|工学|陽子|宮|千里|別|年上|体|医学|存|明日|葉|粉|柄|頸|宗|桂|灰|使|日中|歩|科学|聖人|大手|枝|判|吹|施工|強|宿主|法学|好|膝|介|歩兵|一昨年|強力|早|三国|八幡|尺|定家|前|鶏|一味|酔|分別|難治|氷|小学|前駆|裂|風|半月|分間|麗|膠|竜馬|細目|入|栄|児|粋|兵衛|節|管|風車|頭数|雲|露|見透|月|逆手|香|振|山城|雪|花弁|中|油|兄|雑|経緯|古本|合法|大蔵|秋|氏|日向|下手|討|中間|七|哲|魂|表|事|誠|日本|米|問屋|上|清水|高野|牧|物|口|土|至|正|芽|寛|孫|石|助|恋|等|墓石|流|越|桐|重|敬|何|起|時計|命|際|海|化学|太|酒|床|琴|甘藷|枯|声|炎|山河|器|連中|皮|一|様|伝|紀|開眼|宝|銀杏|勝|境|砂|大山|性|虫|側|有|骨|歌|室|時|耳|駿河|間|北方|玩具|元|二十|丈|万|乳|送|衛門|穂|昭|零|哲郎|調|面|底力|通|善|開発|会津|水面|硝子|昨日|緑色|婆|盛土|言|合|編|墨|漁場|陰|源|咳|縁|一行|英文|明清|二重|着|来|筆|借家|信|張|一時|誰|異|静|依存|血症|末|法典|岳|当|電場|梅雨|探|打|墳|相乗|翡翠|望|上顎|魚|荷|語|抱|馳|極|清|巌|聖|技|森|侍|球|女|羽|坊|教|菖蒲|徹|上方|往|彦|緑|候|三角|固|幼|仏|及|下野|宅|武|遺言|九|大勢|福島|翼|黒子|復|緑化|手続|孝|民|輝|赤血|病|係|分|一昨日|母|内|報告|暮|人|世|鬼|決|大和|真|久|勇|兵馬|他|小判|度|堤|厚|嫂|今日|登|小六|古|種|明代|巣|其|火|一言|宏|年|皆|君|剛|雅|花崗|変化|吾妻|赤|袋|里|余|港|淳|獅子|呉|冷水|所謂|鑑|金|鋼板|発足|常|転生|草|疾風|辺|池|墓|巻|綿|小形|角|格付|十八番|治|糸|布|街|観|紙|水|恵|愛|傍|朝|貫|無|部屋|村|日|国立|古今|桜|黄色|修|小|後|額|酒類|指|空|泉|狼|要|貝|四十|仔|薬|広|隠岐|背|四|研究|一途|玉|童|武蔵|石巻|刀|頭蓋|音|噺|本|拍子|公|寺|動力|類聚|殿|館|足跡|鍼|腹|画|達|匹|書|毛|駕|出展|偽|上下|為|実|男|燃|場|教化|姉|歪|鏡|胸|印|附|働|眸|寒気|西郷|司|菓子|程|気骨|世論|末期|人妻|谷間|草紙|寿|歳|基|大社|横|組|山村|灯|本書|志|悪|伸子|求道|底|心肺|高|蔵|戦|大人|会|馬|落葉|雄|頃|訳|競売|人気|茂|二人|町|悲|原|之|平|修業|大分|秘|史学|木|杯|佐|城跡|仮名|夫婦|抜|問題|二|峰|主|子規|紅葉|彼方|空力|行|白石|熱力|貧|付|動学|一文|明後日|手指|因|手塚|者|風穴|平野|浮|孔|譜|大事|乾|楽|奴|留|創|陽|山陰|生|胎仔|国|三千|紅|独|路|足|倉|品|読|吾|包|秦|沼津|動向|徒然草|方|栄三郎|動静|経|聖徳|日間|施業|保|発|筋|房|裏|頭|沢庵|増|銭|芳|夜話|如|根本|口腔|利益|店|網|嚥下|妻|百|活|権|札|何時|現世|読本|型|大家|十|代|谷|文書|麻|業|形|作法|得|町家|貴女|陰陽|木質|茶道|豚|蚕|帯|千|一方|冬|浪漫|邦|波|心中|味|便|高村|牧場|詩|切|洲|石綿|夢|俊|燕|幻|棟|敷|梁|生物|根治|金色|背筋|大|塚|雷|関|残存|竜|熱|樹|翁|冠|施行|防錆|一目|捧|左|八|問|西|丁|大谷|小倉|草地|笠|答|文学|一分|播\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ef3ba-1701-4241-99e3-3b41d7c0e220",
   "metadata": {},
   "source": [
    "We look in our sentence data for these known heteronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2ec62-9bc3-45e2-bee7-554e6da0ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "full_df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"all_broken_down.csv\"))\n",
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25f447-3ac9-4e59-b29e-45d5b284d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = full_df[\n",
    "    full_df[\"sentence\"].str.contains(heteronyms)\n",
    "]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7867b-f5d9-44c4-ad04-51bbeb8c3dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yomikata import utils\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "import random\n",
    "\n",
    "heteronym_dict = {}\n",
    "dictionary_df = pd.read_csv(Path(config.READING_DATA_DIR, \"all.csv\"))\n",
    "dictionary_set = set(dictionary_df.itertuples(index=False, name=None))\n",
    "\n",
    "# for heteronym in [\"有\"]:\n",
    "for heteronym in heteronyms.split(\"|\"):\n",
    "    furis = df.loc[df[\"sentence\"].str.contains(heteronym), \"furigana\"].values\n",
    "    readings = []\n",
    "    for furi in furis:\n",
    "        reading_list = utils.get_all_surface_readings(heteronym, furi)\n",
    "        readings += reading_list\n",
    "        # readings += [string for string in reading_list if \"ー\" not in string and (heteronym, string) in dictionary_set]\n",
    "    ms = Counter(readings)\n",
    "    ms = {k: v for k, v in sorted(ms.items(), key=lambda item: item[1], reverse=True)}\n",
    "    print(heteronym)\n",
    "    print(ms)\n",
    "    heteronym_dict[heteronym] = ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6100d4-ac9f-4404-86d6-7fc2d453652d",
   "metadata": {},
   "source": [
    "We give up on identifying readings for which we have less than 40 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df3667-a666-4cbe-b12b-a7b743352588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncut = 40\n",
    "heteronym_dict_cut = {\n",
    "    k: {k2: v2 for (k2, v2) in v.items() if v2 > ncut}\n",
    "    for (k, v) in heteronym_dict.items()\n",
    "}\n",
    "heteronym_dict_cut = {k: v for (k, v) in heteronym_dict_cut.items() if len(v) > 1}\n",
    "print(len(heteronym_dict_cut))\n",
    "heteronym_dict_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df04397-e376-4ad3-8356-60f527caff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_dict(heteronym_dict_cut, Path(config.CONFIG_DIR, \"heteronyms.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414fbb7-3584-4e6d-948e-c13cb326b8b5",
   "metadata": {},
   "source": [
    "# Prepare augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157613e-54ae-4b9f-908e-4b08fc09b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "from yomikata import utils\n",
    "\n",
    "input_files = [\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"aozora.csv\"),\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"kwdlc.csv\"),\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"bccwj.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"ndlbib.csv\"),\n",
    "]\n",
    "\n",
    "utils.merge_csvs(input_files, Path(config.SENTENCE_DATA_DIR, \"augmentation.csv\"), n_header=1)\n",
    "logger.info(\"✅ Merged sentence data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc14d4e-a1d6-459d-aca3-6e931352c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"augmentation.csv\"))\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sentence'], keep='first')\n",
    "df_no_duplicates.to_csv(Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered.csv\"), index=False)\n",
    "logger.info(\"✅ Filtered out duplicate sentences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85d985-5d1b-40d9-be9c-504405b66da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "augmentation_filtered_path = Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered.csv\")\n",
    "test_optimized_path = Path(config.SENTENCE_DATA_DIR, \"test/test_optimized_strict_heteronyms.csv\")\n",
    "augmentation_filtered_cleaned_path = Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned.csv\")\n",
    "\n",
    "df_augmented = pd.read_csv(augmentation_filtered_path)\n",
    "df_test_optimized = pd.read_csv(test_optimized_path)\n",
    "\n",
    "initial_row_count = len(df_augmented)\n",
    "\n",
    "df_cleaned = df_augmented[~df_augmented['sentence'].isin(df_test_optimized['sentence'])]\n",
    "\n",
    "final_row_count = len(df_cleaned)\n",
    "\n",
    "logger.info(f\"🔢 Number of rows removed: {initial_row_count - final_row_count}\")\n",
    "\n",
    "df_cleaned.to_csv(augmentation_filtered_cleaned_path, index=False)\n",
    "\n",
    "logger.info(\"✅ Cleaned augmentation_filtered.csv and saved to augmentation_filtered_cleaned.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe970ee-a927-44e1-b4b9-9a63676e7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dataset.split import (\n",
    "    check_data,\n",
    "    filter_dictionary,\n",
    "    filter_simple,\n",
    "    optimize_furigana,\n",
    "    remove_other_readings,\n",
    "    split_data,\n",
    ")\n",
    "\n",
    "logger.info(\"Rough filtering for sentences with heteronyms\")\n",
    "filter_simple(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms.csv\"),\n",
    "    config.HETERONYMS.keys(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6f714-27af-4280-866d-24a2c1751c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.dataset import split\n",
    "from yomikata import utils\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "split_dict = utils.load_dict(Path(config.BREAKDOWN_DATA_DIR, \"translations.json\"))\n",
    "logger.info(\"Starting decomposition process, this may take a while...\")\n",
    "split.decompose_furigana(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down.csv\"),\n",
    "    split_dict,\n",
    ")\n",
    "logger.info(\"✅ Decomposed furigana!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354b8d0-e834-4096-aae2-a91e26d5535c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dataset.split import (\n",
    "    check_data,\n",
    "    filter_dictionary,\n",
    "    filter_simple,\n",
    "    optimize_furigana,\n",
    "    remove_other_readings,\n",
    "    split_data,\n",
    ")\n",
    "logger.info(\"Removing heteronyms with unexpected readings\")\n",
    "remove_other_readings(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down_strict.csv\"),\n",
    "    config.HETERONYMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a9b22-cf05-411d-84d4-c80617aa2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "from yomikata import utils\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down_strict.csv\"))\n",
    "df1 = df1[['sentence', 'furigana']]\n",
    "temp_file = Path(config.SENTENCE_DATA_DIR, \"temp_filtered.csv\")\n",
    "df1.to_csv(temp_file, index=False)\n",
    "\n",
    "input_files = [\n",
    "    temp_file,\n",
    "    Path(config.SENTENCE_DATA_DIR, \"train/train_optimized_strict_heteronyms.csv\"),\n",
    "]\n",
    "utils.merge_csvs(input_files, Path(config.SENTENCE_DATA_DIR, \"train/train_optimized_strict_heteronyms_augmented.csv\"), n_header=1)\n",
    "\n",
    "temp_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81b298-b8c6-4614-8bc4-7b4993dddf29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb97b00-1bef-4ab4-9fae-b7d23a6f5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dataset.split import (\n",
    "    check_data,\n",
    "    filter_dictionary,\n",
    "    filter_simple,\n",
    "    optimize_furigana,\n",
    "    remove_other_readings,\n",
    "    split_data,\n",
    ")\n",
    "from yomikata.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64f3e4-ec6e-42fa-b67b-b89482dda19b",
   "metadata": {},
   "source": [
    "We extract from the dataset the sentences which include our heteronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc7ee9-e135-47ec-b3ad-7a1be21789e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Rough filtering for sentences with heteronyms\")\n",
    "filter_simple(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"all_broken_down.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    config.HETERONYMS.keys(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7286c-4ab0-4be8-a15e-9c59362b11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Use sudachi to filter out heteronyms in known compounds\")\n",
    "filter_dictionary(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    config.HETERONYMS.keys(),\n",
    "    Dictionary(\"sudachi\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b4476-1b01-43f1-906c-535da48224f7",
   "metadata": {},
   "source": [
    "Finally we remove sentences that only include heteronyms with readings that we are not trying to predict for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2279ac4-187f-49cf-ab66-207d0c8656a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info(\"Removing heteronyms with unexpected readings\")\n",
    "remove_other_readings(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"optimized_strict_heteronyms.csv\"),\n",
    "    config.HETERONYMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b6f02-49f3-4b31-b952-2fcecc352bbe",
   "metadata": {},
   "source": [
    "After checking our data makes sense we do a train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a97b3c-ffed-4f2e-942a-6207b753e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = check_data(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"optimized_strict_heteronyms.csv\")\n",
    ")\n",
    "logger.info(\"Performing train/test/split\")\n",
    "split_data(Path(config.SENTENCE_DATA_DIR, \"optimized_strict_heteronyms.csv\"))\n",
    "\n",
    "logger.info(\"Data splits successfully generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ae3b3-7ead-4311-9976-0b4ee4a3e983",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89903d0d-755a-4f15-a833-a099301f5426",
   "metadata": {},
   "source": [
    "We train a BERT classifier model to disambiguate the heteronyms in our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5354776-36a3-4069-8beb-3be415f3eefe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68218fd1-df51-4865-a5b1-5528f25eabd7",
   "metadata": {},
   "source": [
    "Before we start training we do some simple tests using the BERT tokenizer on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91833d-cb8c-4a53-96bb-147c1f16b29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": str(Path(config.TRAIN_DATA_DIR, \"train_optimized_strict_heteronyms.csv\")),\n",
    "        \"val\": str(Path(config.VAL_DATA_DIR, \"val_optimized_strict_heteronyms.csv\")),\n",
    "        \"test\": str(Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")),\n",
    "    },\n",
    ")\n",
    "from yomikata.dbert import dBert\n",
    "\n",
    "reader = dBert()\n",
    "\n",
    "dataset = dataset.map(\n",
    "    reader.batch_preprocess_function, batched=True, fn_kwargs={\"pad\": False}\n",
    ")\n",
    "dataset = dataset.filter(\n",
    "    lambda entry: any(label != -100 for label in entry[\"labels\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0555463-b3de-47f7-8fd7-f4608eae1426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "labels = []\n",
    "for key in dataset.keys():\n",
    "    print(f\"{key} dataset has {len(dataset[key])} members\")\n",
    "    have_labels = [i for i in dataset[key] if np.max(i[\"labels\"]) != -100]\n",
    "    print(f\"{len(have_labels)} actually contain heteronyms\")\n",
    "    key_length = len(dataset[key])\n",
    "    for i in tqdm(range(key_length), desc=\"Counting labels\"):\n",
    "        labels += [value for value in dataset[key][i][\"labels\"] if value != -100]\n",
    "    print(\"--\")\n",
    "\n",
    "label_counter = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e20814-91ec-4b82-a423-ed0fa9d0a0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "heteronyms = defaultdict(dict)\n",
    "\n",
    "for label in label_counter:\n",
    "    label_class = reader.label_encoder.index_to_class[label]\n",
    "    (surface, reading) = label_class.split(\":\")\n",
    "    heteronyms[surface][reading] = label_counter[label]\n",
    "\n",
    "for heteronym in reader.heteronyms:\n",
    "    print(\"heteronym:\", heteronym)\n",
    "    total = 0\n",
    "    for reading in heteronyms[heteronym]:\n",
    "        print(reading, heteronyms[heteronym][reading])\n",
    "        total += heteronyms[heteronym][reading]\n",
    "    print(\"total:\", total)\n",
    "    print(\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa92840-6476-4da1-9a2d-e92a09de33d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccac467-2d55-43a1-bbb4-799ca1c8f4ed",
   "metadata": {},
   "source": [
    "To train the model in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc67c4b-6346-434b-9ccd-ec50d4db438c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yomikata.dbert import dBert\n",
    "from datasets import load_dataset\n",
    "from yomikata.config import config, logger\n",
    "from pathlib import Path\n",
    "\n",
    "reader = dBert(reinitialize=True)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": str(Path(config.TRAIN_DATA_DIR, \"train_optimized_strict_heteronyms.csv\")),\n",
    "        \"val\": str(Path(config.VAL_DATA_DIR, \"val_optimized_strict_heteronyms.csv\")),\n",
    "        \"test\": str(Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")),\n",
    "    },\n",
    ")\n",
    "\n",
    "reader.train(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38438d88-e9fd-453b-b6fd-40368b60cc59",
   "metadata": {},
   "source": [
    "Or using to get MLflow integration, experiment tracking, metrics, run the following in command line:\n",
    "\n",
    "```\n",
    "source yomikata/venv/bin/activate\n",
    "\n",
    "python yomikata/yomikata/main.py yomikata/config/dbert-train-args.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40615f12-cd82-461c-bf5f-b0992648a43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa9054-8dfd-4c17-97b9-c5aca73e7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dbert import dBert\n",
    "# from yomikata.main import get_artifacts_dir_from_run\n",
    "\n",
    "# artifacts_dir = get_artifacts_dir_from_run(\"e392694b345e4ca19fd97f6a872ced98\")\n",
    "# reader = dBert(artifacts_dir)\n",
    "reader = dBert()\n",
    "\n",
    "from yomikata.dictionary import Dictionary\n",
    "\n",
    "dictreader = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087023-f28c-4960-b12c-114b81cc9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"知って備える新型インフルエンザ職場・家庭で今日からすべきこと\"  # 知[し]って備[そな]える新型[しんがた]インフルエンザ職場[しょくば]・家庭[かてい]で今日[きょう]からすべきこと\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f55418-8370-4fb0-87dd-b09b5d32b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"身体--我々自身がそれであるところの自然\"  # 身体[しんたい]--我々[われわれ]自身[じしん]がそれであるところの自然[しぜん]\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba6ba1-459a-4a47-992f-ce79a2735252",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"気がついたものかそれとも偶然からか、狙われた団七がふと首をすくめたので、危うく鉄扇がその身体の上を通り越しながら、丁度並行して大坪流の秘術をつくしつつあった右側向うの、黒住団七ならぬ古高新兵衛の脇腹に、はッしと命中いたしました。\"  # ,気[き]がついたものかそれとも偶然[ぐうぜん]からか、狙[ねら]われた団七[だんしち]がふと首[くび]をすくめたので、危[あや]うく鉄扇[てっせん]がその身体[からだ]の上[うえ]を通[とお]り越[こ]しながら、丁度[ちょうど]並行[へいこう]して大坪流[おおつぼりゅう]の秘術[ひじゅつ]をつくしつつあった右側[みぎがわ]向[むこ]うの、黒住[くろずみ]団七[だんしち]ならぬ古高[ふるたか]新兵衛[しんべえ]の脇腹[わきばら]に、はッしと命中[めいちゅう]いたしました。\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a555bad-c5ed-484d-b3b5-fa2f583d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Interview雲田はるこ:BLから『昭和元禄落語心中』まで人間の個性を見つめる稀代の描き手\"  # ,Interview雲田[うんでん]はるこ:BLから『昭和[しょうわ]元禄[げんろく]落語[らくご]心中[しんじゅう]』まで人間[にんげん]の個性[こせい]を見[み]つめる稀代[きたい]の描[えが]き手[て]\n",
    "reader.furigana(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d29a28-06cf-4dfc-a741-89320323f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"特集生成する身体\"  # ,特集[とくしゅう]生成[せいせい]する身体[しんたい]\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a91ce1-4a8b-4447-9fb7-68bff37dfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"今日の世界情勢は\"\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc58803-f63a-4a16-80c1-f0a4294ec195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"あの力士には金星はどれぐらいある？\"\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610528e3-11ed-4a67-b757-ad78d13ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"黄色と黒の組み合わせは、危険であることを表す\"\n",
    "reader.furigana(text)  # 表　is in but 表す　is properly parsed as not ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d57bf-ef34-45f1-9fd8-72ec19a859c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"表参道に行きます\"\n",
    "reader.furigana(\n",
    "    text\n",
    ")  # 表　is in but since is in the compound 表参道　 it is properly recognized as something that should be looked up in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369962d2-9395-461f-bbd9-aa7d7ea1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"その表を見せてください\"\n",
    "reader.furigana(text)  # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4350f7-e4cb-4d8c-a30f-34a7727e75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"あの家の表は綺麗です\"\n",
    "reader.furigana(text)  # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266f685-837a-4b2d-b802-9fb4ae0af93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"建築表を見せてください\"\n",
    "reader.furigana(text)  # Failed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60451c5-8c54-4057-a0b4-e2f1fb8547fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374ab2d-29d0-4c3e-80de-d7545e3c27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata import utils\n",
    "from yomikata.dbert import dBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254f905-cede-4ffd-941a-39bf8c4cefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = dBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013e51e-9bb7-481c-bc7b-e7f7e6679718",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'そして、{畳/たたみ}の{表/おもて}は、すでに{幾/いく}{年/ねん}{前/まえ}に{換/か}えられたのか{分/わか}らなかった'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8eb00-259f-4c7d-9a21-8cddf11a7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "disambiguated_sentence = reader.furigana(utils.remove_furigana(test_sentence))\n",
    "print(disambiguated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d5009-cc90-45d4-9c6a-59be622ab019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dictionary import Dictionary\n",
    "\n",
    "dictreader = Dictionary()\n",
    "dictreader.furigana(utils.remove_furigana(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f302515-926f-40cb-9c97-8fba20dec1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictreader.furigana(disambiguated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd3afd-f4c6-4c89-bc8a-59fe3254ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictreader.furigana(disambiguated_sentence) == dictreader.furigana(\n",
    "    utils.remove_furigana(test_sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804737b-4dbe-435c-9736-eeca60b40dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055569b-07dc-42fc-930f-6ab6e4be5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from yomikata.config import config\n",
    "from yomikata.dbert import dBert\n",
    "from yomikata.main import get_artifacts_dir_from_run\n",
    "\n",
    "# artifacts_dir = get_artifacts_dir_from_run(\"e392694b345e4ca19fd97f6a872ced98\")\n",
    "# artifacts_dir = Path(\n",
    "#    get_artifacts_dir_from_run(\"4d19dfb0d0b64b518d8e5506e3f6a726\"), \"checkpoint-10200\"\n",
    "# )\n",
    "\n",
    "reader = dBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fc125-0911-4cd1-8b26-97cc09ec5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": str(\n",
    "            Path(config.TRAIN_DATA_DIR, \"train_optimized_strict_heteronyms.csv\")\n",
    "        ),\n",
    "        \"val\": str(Path(config.VAL_DATA_DIR, \"val_optimized_strict_heteronyms.csv\")),\n",
    "        \"test\": str(Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")),\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    reader.batch_preprocess_function, batched=True, fn_kwargs={\"pad\": False}\n",
    ")\n",
    "dataset = dataset.filter(\n",
    "    lambda entry: any(label != -100 for label in entry[\"labels\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7017bc-9b98-43ef-96f9-a3011605e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from yomikata.custom_bert import CustomDataCollatorForTokenClassification\n",
    "import evaluate\n",
    "\n",
    "data_collator = CustomDataCollatorForTokenClassification(\n",
    "    tokenizer=reader.tokenizer, padding=True\n",
    ")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p  # predictions are already the argmax of logits\n",
    "    true_predictions = [pred for prediction, label in zip(predictions, labels) for pred, lab in zip(prediction, label) if lab != -100]\n",
    "    true_labels = [lab for prediction, label in zip(predictions, labels) for pred, lab in zip(prediction, label) if lab != -100]\n",
    "    return {\"accuracy\": accuracy_metric.compute(references=true_labels, predictions=true_predictions)[\"accuracy\"], \"recall\": recall_metric.compute(references=true_labels, predictions=true_predictions, average=\"macro\", zero_division=0)[\"recall\"]}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=reader.model,\n",
    "    tokenizer=reader.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=lambda logits, _: torch.argmax(logits, dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd520e0a-0a22-40e6-9678-6dabb9ada96d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from yomikata.config import logger\n",
    "\n",
    "reader.model.eval()\n",
    "full_performance = {}\n",
    "# for key in dataset.keys():\n",
    "for key in [\"test\"]:\n",
    "    max_evals = min(1000000, len(dataset[key]))\n",
    "    # max_evals = len(dataset[key])\n",
    "    logger.info(f\"getting predictions for {key}\")\n",
    "    subset = dataset[key].shuffle().select(range(max_evals))\n",
    "    prediction_output = trainer.predict(subset)\n",
    "    logger.info(f\"processing predictions for {key}\")\n",
    "    metrics = prediction_output[2]\n",
    "    labels = prediction_output[1]\n",
    "\n",
    "    logger.info(\"processing performance\")\n",
    "    performance = {\n",
    "        heteronym: {\n",
    "            \"n\": 0,\n",
    "            \"readings\": {\n",
    "                reading: {\n",
    "                    \"n\": 0,\n",
    "                    \"found\": {readingprime: 0 for readingprime in list(reader.heteronyms[heteronym].keys())}\n",
    "                }\n",
    "                for reading in list(reader.heteronyms[heteronym].keys())\n",
    "            },\n",
    "        }\n",
    "        for heteronym in reader.heteronyms.keys()\n",
    "    }\n",
    "\n",
    "    flattened_logits = [\n",
    "        logit\n",
    "        for sequence_logits, sequence_labels in zip(prediction_output[0], labels)\n",
    "        for (logit, l) in zip(sequence_logits, sequence_labels) if l != -100\n",
    "    ] # this is already argmaxed in preprocess_logits_for_metrics, so the resulting list is 1d. valid_mask processing in CustomBertForTokenClassification.forward takes care of zeoring out irrelevant logits\n",
    "\n",
    "    true_labels = [\n",
    "        str(reader.label_encoder.index_to_class[l])\n",
    "        for label in labels\n",
    "        for l in label if l != -100\n",
    "    ]\n",
    "\n",
    "    for i, true_label in enumerate(true_labels):\n",
    "        (true_surface, true_reading) = true_label.split(\":\")\n",
    "        performance[true_surface][\"n\"] += 1\n",
    "        performance[true_surface][\"readings\"][true_reading][\"n\"] += 1\n",
    "        predicted_label = reader.label_encoder.index_to_class[flattened_logits[i]]\n",
    "        predicted_reading = predicted_label.split(\":\")[1]\n",
    "        performance[true_surface][\"readings\"][true_reading][\"found\"][predicted_reading] += 1\n",
    "\n",
    "    for surface in performance:\n",
    "        for true_reading in performance[surface][\"readings\"]:\n",
    "            true_count = performance[surface][\"readings\"][true_reading][\"n\"]\n",
    "            predicted_count = performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "            performance[surface][\"readings\"][true_reading][\"accuracy\"] = predicted_count / true_count if true_count > 0 else \"NaN\"\n",
    "        correct_count = sum(performance[surface][\"readings\"][true_reading][\"found\"][true_reading] for true_reading in performance[surface][\"readings\"])\n",
    "        all_count = performance[surface][\"n\"]\n",
    "        performance[surface][\"accuracy\"] = correct_count / all_count if all_count > 0 else \"NaN\"\n",
    "\n",
    "    performance = {\n",
    "        \"metrics\": metrics,\n",
    "        \"heteronym_performance\": performance,\n",
    "    }\n",
    "\n",
    "    full_performance[key] = performance\n",
    "\n",
    "full_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1c56c-27f7-4990-86ed-dc9ba4b53be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Performance for dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04578d8-e2f8-42e0-b4f4-167ba8e995df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yomikata.config import config, logger\n",
    "from speach.ttlig import RubyFrag, RubyToken\n",
    "from yomikata import utils\n",
    "from yomikata.dictionary import Dictionary\n",
    "from yomikata.dataset import breakdown\n",
    "from yomikata.dataset.split import replace_furigana\n",
    "\n",
    "reader = Dictionary(\"sudachi\")\n",
    "heteronyms = config.HETERONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c0d8f-7beb-4999-9f54-12c26a965a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")\n",
    "max_evals = 1000000\n",
    "df = pd.read_csv(filename, header=0)\n",
    "df = df.sample(frac=1)\n",
    "if max_evals is not None:\n",
    "    max_evals = max(max_evals, 1)\n",
    "    max_evals = min(max_evals, len(df))\n",
    "    df = df.head(max_evals)\n",
    "\n",
    "df[\"furigana_found\"] = df.apply(\n",
    "    lambda x: reader.furigana(utils.standardize_text(x[\"sentence\"])), axis=1\n",
    ")\n",
    "\n",
    "sentences = df[\"furigana_found\"].tolist()\n",
    "sentences += df[\"furigana\"].tolist()\n",
    "(split_dict, no_translation) = breakdown.sentence_list_to_breakdown_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202971cd-ad3c-450e-bf6c-33406613e17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"furigana_found\"] = df[\"furigana_found\"].apply(\n",
    "    lambda s: replace_furigana(s, split_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454d861-2578-452c-950f-26091e8bc6a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "performance = {\n",
    "    heteronym: {\n",
    "        \"n\": 0,\n",
    "        \"readings\": {\n",
    "            reading: {\n",
    "                \"n\": 0,\n",
    "                \"found\": {\n",
    "                    readingprime: 0\n",
    "                    for readingprime in list(heteronyms[heteronym].keys()) + [\"<OTHER>\"]\n",
    "                },\n",
    "            }\n",
    "            for reading in list(heteronyms[heteronym].keys())\n",
    "        },\n",
    "    }\n",
    "    for heteronym in heteronyms.keys()\n",
    "}\n",
    "failures = 0\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"processing performance\"):\n",
    "    matches = utils.find_all_substrings(row[\"sentence\"], heteronyms.keys())\n",
    "    furis_true = utils.get_furis(row[\"furigana\"])\n",
    "    furis_found = utils.get_furis(row[\"furigana_found\"])\n",
    "    failure = False\n",
    "    for location in matches:\n",
    "        surface = matches[location]\n",
    "        reading_true = utils.get_reading_from_furi(location, len(surface), furis_true)\n",
    "        if not reading_true:\n",
    "            continue\n",
    "        reading_found = utils.get_reading_from_furi(location, len(surface), furis_found)\n",
    "        if not reading_found:\n",
    "#            print(location, surface, row[\"furigana\"], row[\"furigana_found\"], row[\"sentence\"])\n",
    "            failure = True\n",
    "        performance[surface][\"n\"] += 1\n",
    "        if (reading_true in performance[surface][\"readings\"].keys()):\n",
    "            found_reading = reading_found if reading_found in performance[surface][\"readings\"].keys() else \"<OTHER>\"\n",
    "            performance[surface][\"readings\"][reading_true][\"n\"] += 1\n",
    "            performance[surface][\"readings\"][reading_true][\"found\"][found_reading] += 1\n",
    "    if failure:\n",
    "        failures += 1\n",
    "n = 0\n",
    "correct = 0\n",
    "for surface in performance.keys():\n",
    "    for true_reading in performance[surface][\"readings\"].keys():\n",
    "        performance[surface][\"readings\"][true_reading][\"accuracy\"] = np.round(\n",
    "            performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "            / np.array(performance[surface][\"readings\"][true_reading][\"n\"]),\n",
    "            3,\n",
    "        )\n",
    "\n",
    "    performance[surface][\"accuracy\"] = np.round(\n",
    "        sum(\n",
    "            performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "            for true_reading in performance[surface][\"readings\"].keys()\n",
    "        )\n",
    "        / np.array(performance[surface][\"n\"]),\n",
    "        3,\n",
    "    )\n",
    "\n",
    "    correct += sum(\n",
    "        performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "        for true_reading in performance[surface][\"readings\"].keys()\n",
    "    )\n",
    "    n += performance[surface][\"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4f0e8-5fbd-4017-b060-410a5f5245c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(failures, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b9f9e-b859-4144-b8a7-e7178590be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total accuracy:\", correct/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686759e-bf78-4ea8-a519-41f8fab8b570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print({key: performance[key][\"accuracy\"] for key in performance.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc6caa-7cdc-4d5f-ac45-16f4bf69ffbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efa2da-01a3-4207-a9d4-b15af9812303",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Details of classifying based on textual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea2daf-850e-464a-99f1-638e1ee2f29a",
   "metadata": {},
   "source": [
    "With the T5 model I am fine-tuning the whole encoder-decoder architecture to encode the embeddings and then output the correct readings for every token. This assumes essentially that every token can be ambiguous and can have any possible reading.\n",
    "\n",
    "The Amazon paper does something simpler. It takes the BERT encodings as input and for ambiguous tokens trains a small classifier model to choose between 2 or 3 readings. Is this kind of thing possible for Japanese? Let's look at some tokenizations and see if such a thing is possible for japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769be0a1-7362-47ed-8839-737e8fdefe73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Proof of concept: Do contextual embeddings significantly differ for heteronyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac06b92-8c5c-47c2-8ab8-75cb1a0af1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"金星\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36b8d9-2807-4d2a-a16d-0ab4527c7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"金星は太陽系で太陽に近い方から2番目の惑星。\"\n",
    "text2 = \"金星とは、大相撲で、平幕の力士が横綱と取組をして勝利することである。\"\n",
    "texts = [text1, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e65a0-8c86-4355-8b26-1f8ecfcb007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dictionary import DictionaryReader\n",
    "\n",
    "DicReader = DictionaryReader()\n",
    "for text in texts:\n",
    "    print(DicReader.tagger(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc38d4-6661-4d74-867e-d43672cbfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on tokenizer results below 平幕 appears to be in unidic but not unidic_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f67b2-21bd-439c-bc84-2d62f11ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d66c9-5185-4dae-9129-1a8abd2e1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "for text in texts:\n",
    "    text_encoded = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_ids = text_encoded[\"input_ids\"]\n",
    "    input_mask = text_encoded[\"attention_mask\"]\n",
    "    print(input_ids)\n",
    "    print([tokenizer._convert_id_to_token(input_id) for input_id in input_ids])\n",
    "    tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1d702-5f4a-4247-a5ea-707a8282d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d064380-2d09-484e-b1a9-3ba35c9cf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    text_encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=16,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "    )  # needs to be pytorch tensors\n",
    "    input_ids = text_encoded[\"input_ids\"]\n",
    "    input_mask = text_encoded[\"attention_mask\"]\n",
    "\n",
    "    print(input_ids.shape)\n",
    "\n",
    "    outputs = model.forward(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "    print(outputs.last_hidden_state)\n",
    "    print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067a8d4-df70-4045-a993-ef5b7fcb78b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef289a-5389-43e2-9ca3-5a14ca71dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model.eval()\n",
    "import numpy as np\n",
    "\n",
    "words = np.array(list(tokenizer.vocab.keys()))\n",
    "wordembs = model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b7766-a699-4737-b9ac-21fabb8de3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordembs.shape)  # 32768 is the vocab size and 768 the embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26f5dd-1de0-42ec-81fc-064c45f1678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs = wordembs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fbcab-997b-48b9-b0ab-a0421f7c9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine vocabulary to use for t-SNE/visualization. The indices are hard-coded based partially on inspection:\n",
    "char_indices_to_use = np.arange(851, 1063, 1)\n",
    "voc_indices_to_plot = np.append(char_indices_to_use, np.arange(23000, 27000, 1))\n",
    "voc_indices_to_use = np.append(char_indices_to_use, np.arange(17000, 27000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199accac-416f-48b8-846f-e2cc157a497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(voc_indices_to_plot))\n",
    "print(len(voc_indices_to_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56655c71-8241-49ba-b3b2-15b371b2ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(words[bert_voc_indices_to_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a13f39-291d-4353-b093-3a3b45a9c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs_to_use = wordembs[voc_indices_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1710b-bbb0-42e9-a66b-14a8a9d80966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Run t-SNE on the BERT vocabulary embeddings we selected:\n",
    "mytsne_words = TSNE(n_components=2, early_exaggeration=12, metric=\"cosine\", init=\"pca\")\n",
    "wordembs_to_use_tsne = mytsne_words.fit_transform(wordembs_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95547c-97bc-4198-844d-ed946e25d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs_to_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fce6e7-1c62-4ac5-9127-42d63243b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213726c1-11de-45cb-a3bd-f17e3e5c1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_plot = words[voc_indices_to_plot]\n",
    "print(len(words_to_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1b3b8-7022-4538-90c9-61675bd1dc12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the transformed BERT vocabulary embeddings:\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"VL Gothic\"\n",
    "\n",
    "fig = plt.figure(figsize=(100, 60))\n",
    "alltexts = list()\n",
    "for i, txt in enumerate(words_to_plot):\n",
    "    plt.scatter(wordembs_to_use_tsne[i, 0], wordembs_to_use_tsne[i, 1], s=0)\n",
    "    currtext = plt.text(wordembs_to_use_tsne[i, 0], wordembs_to_use_tsne[i, 1], txt)\n",
    "    alltexts.append(currtext)\n",
    "\n",
    "\n",
    "# Save the plot before adjusting.\n",
    "plt.savefig(\"japanese-viz-bert-voc-noadj.pdf\", format=\"pdf\")\n",
    "# print('now running adjust_text')\n",
    "# Using autoalign often works better in my experience, but it can be very slow for this case, so it's false by default below:\n",
    "# numiters = adjust_text(alltexts, autoalign=True, lim=50)\n",
    "# from adjustText import adjust_text\n",
    "# numiters = adjust_text(alltexts, autoalign=False, lim=50)\n",
    "# print('done adjust text, num iterations: ', numiters)\n",
    "# plt.savefig('japanese-viz-bert-voc-tsne10k-viz4k-adj50.pdf', format='pdf')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23952e8-e189-4048-9414-c2100721897b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### フォント一覧を確認するサンプルコード\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "# import numpy as np\n",
    "\n",
    "# fonts = list(np.unique([f.name for f in matplotlib.font_manager.fontManager.ttflist]))\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 100))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.set_ylim([-1, len(fonts)])\n",
    "# ax.set_yticks(np.arange(0, len(fonts), 10))\n",
    "\n",
    "# for i, f in enumerate(fonts):\n",
    "#     ax.text(0.2, i,  '日本語強 {}'.format(f), fontdict={'family': f, 'fontsize': 14})\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee5648-4242-47a8-91b0-c4626a94c891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"aozora.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ff09c-9c4c-4383-9fd7-aa1c27708fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"市場\"\n",
    "word_classes = [\"しじょう\", \"いちば\"]\n",
    "word = \"礼拝\"\n",
    "word_classes = [\"れいはい\", \"らいはい\"]\n",
    "word = \"今日\"\n",
    "word_classes = [\"きょう\", \"こんにち\"]\n",
    "word = \"今日\"\n",
    "word_classes = [\"きょう\", \"こんにち\"]\n",
    "word = \"表\"\n",
    "word_classes = [\"ひょう\", \"おもて\"]\n",
    "word = \"仮名\"\n",
    "word_classes = [\"かな\", \"かめい\"]\n",
    "word = \"変化\"\n",
    "word_classes = [\"へんか\", \"へんげ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d48620-38f1-4441-a791-6b27d2609ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.heteronyms import heteronyms\n",
    "\n",
    "print(heteronyms[heteronyms[\"surface\"] == word])\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "pronunciation_df = pd.read_csv(Path(config.PRONUNCIATION_DATA_DIR, \"all.csv\"))\n",
    "print(pronunciation_df[pronunciation_df[\"surface\"] == word][\"pronunciations\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e53b35-0e8e-400c-9b84-2aae56cc7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = df[df[\"sentence\"].str.contains(word)]\n",
    "df_keyword = df_keyword.reset_index(drop=True)\n",
    "window_size = 128\n",
    "df_keyword[\"sentence-shorter\"] = df_keyword[\"sentence\"].apply(\n",
    "    lambda sentence: (\n",
    "        idx := sentence.index(word),\n",
    "        sentence[np.max([0, idx - window_size]) : idx]\n",
    "        + sentence[idx : np.min([len(sentence), idx + window_size])],\n",
    "    )[1]\n",
    ")\n",
    "print(len(df_keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58e01a-45a6-42ff-9053-ba9cb0762879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_matcher(furigana, word, word_classes):\n",
    "    try:\n",
    "        shifted_furigana = furigana[furigana.index(word) :]\n",
    "    except ValueError:\n",
    "        print(word)\n",
    "        print(furigana)\n",
    "        return -1\n",
    "    found_reading = shifted_furigana[\n",
    "        shifted_furigana.index(\"[\") + 1 : shifted_furigana.index(\"]\")\n",
    "    ]\n",
    "    # print(found_reading)\n",
    "    for reading in word_classes:\n",
    "        if found_reading.find(reading) != -1:\n",
    "            return reading\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d45c3-ba5d-40f2-b122-88f0132c8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword[\"reading\"] = df_keyword[\"furigana\"].apply(\n",
    "    lambda sentence: reading_matcher(sentence, word, word_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d65ab-952b-4dee-ba61-b82bbf15a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve the code for classifying words with furigana into one of the reading classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b1f0-57bf-414f-b59f-2cc25b0598a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_class in word_classes:\n",
    "    print(f\"{word_class} {len(df_keyword[df_keyword['reading'] == word_class])}\")\n",
    "print(\"failures\", len(df_keyword[df_keyword[\"reading\"] == -1]))\n",
    "df_keyword[df_keyword[\"reading\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d121819-653a-4d9f-b320-6fa6ed7f1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = df_keyword[df_keyword[\"reading\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc679c-d319-4092-8638-57660ecc5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "pad_size = 32\n",
    "df_keyword[\"sentence-encoded\"] = df_keyword[\"sentence-shorter\"].apply(\n",
    "    lambda sentence: tokenizer.encode(\n",
    "        sentence,\n",
    "        add_special_tokens=False,\n",
    "        max_length=pad_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    ")\n",
    "df_keyword[\"encoding-success\"] = df_keyword[\"sentence-encoded\"].apply(\n",
    "    lambda encoding: word_id in encoding\n",
    ")\n",
    "print(len(df_keyword[~df_keyword[\"encoding-success\"]]), \"encoding failures\")\n",
    "df_keyword = df_keyword[df_keyword[\"encoding-success\"]]\n",
    "df_keyword = df_keyword.reset_index(drop=True)\n",
    "df_keyword[\"keyword-index\"] = df_keyword[\"sentence-encoded\"].apply(\n",
    "    lambda encoding: encoding.index(word_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c555d-140d-4bc7-b6c6-994b12993ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_keyword[\"keyword-index\"] = df_keyword[\"sentence-encoded\"].apply(\n",
    "    lambda encoding: encoding.index(word_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20889a9-925e-4b2b-bf2c-671c76495ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_stack = np.vstack(df_keyword[\"sentence-encoded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31210184-4fa3-4b86-acf6-5166aa4f5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "forward_pass = model.forward(torch.tensor(encoding_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79da28-1f74-4f72-9c5f-c1ff70ff43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(forward_pass[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79983eb-fd3b-4ba7-adb5-f9906577a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "for i in range(len(df_keyword)):\n",
    "    embs.append(forward_pass[0][i][df_keyword.at[i, \"keyword-index\"]].detach().numpy())\n",
    "embs = np.array(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48403c30-8463-4d31-ac7c-aa20c0aec7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Run t-SNE on the contextualized embeddings:\n",
    "mytsne_tokens = TSNE(\n",
    "    n_components=2,\n",
    "    early_exaggeration=12,\n",
    "    verbose=2,\n",
    "    metric=\"cosine\",\n",
    "    init=\"pca\",\n",
    "    n_iter=2000,\n",
    ")\n",
    "embs_tsne = mytsne_tokens.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3777f6-c567-4920-95d2-71ca0fe1d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the keyword+context strings.\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"VL Gothic\"\n",
    "\n",
    "colors = [\"red\", \"black\", \"blue\", \"green\"]\n",
    "classes = list(df_keyword[\"reading\"].unique())\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "cs = [\n",
    "    colors[classes.index(df_keyword[\"reading\"].iloc[i])] for i in range(len(df_keyword))\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.scatter(embs_tsne[:, 0], embs_tsne[:, 1], s=1, color=cs)\n",
    "\n",
    "plt.savefig(\"japanese-viz-bert-ctx-points-\" + word + \".pdf\", format=\"pdf\")\n",
    "plt.savefig(\"japanese-viz-bert-ctx-points-\" + word + \".png\", format=\"png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c3092-8bd7-4460-a735-6e3ea3aed91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the keyword+context strings.\n",
    "# import matplotlib.pyplot as plt\n",
    "# import japanize_matplotlib\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = \"VL Gothic\"\n",
    "\n",
    "# colors = ['red', 'black']\n",
    "# classes = list(df_keyword['reading'].unique())\n",
    "# fig = plt.figure(figsize=(50, 30))\n",
    "# alltexts = list()\n",
    "# for i, txt in enumerate(df_keyword['sentence-shorter']):\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)\n",
    "#     plt.scatter(embs_tsne[i,0], embs_tsne[i,1], s=0)\n",
    "#     c = colors[classes.index(df_keyword['reading'].iloc[i])]\n",
    "#     currtext = plt.text(embs_tsne[i,0], embs_tsne[i,1], txt, color=c)\n",
    "#     #alltexts.append(currtext)\n",
    "\n",
    "# plt.savefig('japanese-viz-bert-ctx-text-'+word+'.pdf', format='pdf')\n",
    "# # print('now running adjust_text')\n",
    "# #numiters = adjust_text(alltexts, autoalign=True, lim=50)\n",
    "# #numiters = adjust_text(alltexts, autoalign=False, lim=50)\n",
    "# #print('done adjust text, num iterations: ', numiters)\n",
    "# #plt.savefig('viz-bert-ctx-values-viz750-adj.pdf', format='pdf')\n",
    "\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6255d6-1c9c-40c0-92a3-3bdcf7815a88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Handling out of vocab heteronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403dbad-506b-475c-9723-81997a12bc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"その力士には金星が多くて大人気。\"\n",
    "text = \"一時\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c944f-ae1d-48c1-8e8c-8e8e81c51116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dictionary import DictionaryReader\n",
    "\n",
    "DicReader = DictionaryReader()\n",
    "DicReader.tagger(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b537513-532c-4501-8687-30593624ac1c",
   "metadata": {},
   "source": [
    "Here we see a problem: The ambiguous word 大人気 is marked as two tokens. Does bert use the same tokenizer? (It uses unidic-lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966856f9-1cf4-4ae3-9d25-d0b857104943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d0eff-6a06-4c22-8c44-3145f54ef543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "print(input_ids)\n",
    "print([tokenizer._convert_id_to_token(input_id) for input_id in input_ids])\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085fa06-7c67-4801-96ac-9b3befd401d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"一時\" in list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7058518-0af6-4b28-ab94-7269f1f0683d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73c8ae-87f0-4b21-9fb3-4a2258c1382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"一時\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01c1b8-a60d-498f-b85e-908560f9ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"一時\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135ed3b-896f-4121-a73f-fbd5467df197",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583e5f7-6928-4dc6-b076-7a4327c7d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([\"一時\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c96c6-e1fb-4c56-9f92-0f61828080be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode([\"一時\"], add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda98944-2e08-4aee-8ff7-b33c88e4e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b0a11-3371-4200-b64d-3ca86c4bada4",
   "metadata": {},
   "source": [
    "Note this is not a contextual embedding yet, let's look at it after contextualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ce492-b9ae-4e74-aedf-c474d4a67a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc19e91-4ee2-4e05-aa9e-1b3169ffa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    # max_length=4,\n",
    "    # truncation=True,\n",
    "    # padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ")  # needs to be pytorch tensors\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "\n",
    "print(input_ids.shape)\n",
    "\n",
    "outputs = model.forward(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "print(outputs.last_hidden_state)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c1840-a4d1-4cf7-9068-c7e109e01826",
   "metadata": {},
   "source": [
    "Now let's add a word to the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f3bcc-bfaa-4601-a095-47ec1ecb70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([\"大人気\"])\n",
    "model.resize_token_embeddings(\n",
    "    len(tokenizer)\n",
    ")  # Resize the dictionary size of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d69fd5-addb-4997-b1c8-93d9e44c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec157e-b923-41fa-b5b2-c6fca1a2791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "print(input_ids)\n",
    "print([tokenizer._convert_id_to_token(input_id) for input_id in input_ids])\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11ebf0-a5cd-407a-b509-1035a15cffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    # max_length=4,\n",
    "    # truncation=True,\n",
    "    # padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ")  # needs to be pytorch tensors\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "\n",
    "print(input_ids.shape)\n",
    "\n",
    "outputs = model.forward(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "print(outputs.last_hidden_state)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
