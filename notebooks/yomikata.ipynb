{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c78829-e0f9-446d-b7e1-ea56570185ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Yomikata: Disambiguating Japanese Heteronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15e82b-0f6d-4975-9a67-b6a51f2392b7",
   "metadata": {},
   "source": [
    "A step by step guide to training Yomikata's word disambiguation model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edb6478-ff14-49d8-a4e7-2421059efd6e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Word pronunciation lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3db016-1d49-4167-9d74-9832b70965cd",
   "metadata": {},
   "source": [
    "To clean the datasets we use it is useful to have a list of Japanese words and their pronunciations. \n",
    "\n",
    "We do that by parsing the unidic and sudachi dictionaries. Note these scripts are slow -- but run one time only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdca4531-d241-4bee-b30a-531ac1ba56a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.unidic import unidic_data\n",
    "\n",
    "unidic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3bc77c-1d3c-4ffd-b262-195a276d8a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.sudachi import sudachi_data\n",
    "\n",
    "sudachi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592781a9-95ea-4be7-a71d-5ea0df0e9e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.kanjidic import kanjidic_data\n",
    "\n",
    "kanjidic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fba305-77b6-4ce9-a7af-4d5eac389d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.pronunciations import pronunciation_data\n",
    "\n",
    "pronunciation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b059a9-a88f-4fe2-ac27-6c93f4a30387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from yomikata.config import config\n",
    "\n",
    "df = pd.read_csv(Path(config.READING_DATA_DIR, \"all.csv\"))\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e8cbc-d036-405c-8963-4923a151ba0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Corpuses of annotated sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9db4c-9c21-4959-aa09-d96846c46163",
   "metadata": {},
   "source": [
    "The model is trained on sentences which already have furigana. We have four data sources which we process here. Note these scripts are slow -- but run one time only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b586b-1a9d-4c17-95ec-fd9e4bc3e964",
   "metadata": {},
   "source": [
    "[Corpus of titles of works in the national diet library](https://github.com/ndl-lab/huriganacorpus-ndlbib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b16cf5-e703-4d64-aa17-57b22218c7b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from yomikata.dataset.ndlbib import ndlbib_data\n",
    "\n",
    "# ndlbib_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58623bf-e5c9-4193-83c4-e15206a2caa3",
   "metadata": {},
   "source": [
    "[Aozora Bunko book corpus](https://github.com/ndl-lab/huriganacorpus-aozora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e2883-046e-4d4c-9c1e-e7ef3486c58d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.aozora import aozora_data\n",
    "\n",
    "aozora_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18da6a94-b5a9-4bf2-ab1a-9ef0893bd2f6",
   "metadata": {},
   "source": [
    "[Kyoto University document leads corpus](https://github.com/ku-nlp/KWDLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c6008d-3429-4272-8610-22cf4eb47495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.kwdlc import kwdlc_data\n",
    "\n",
    "kwdlc_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47b68e-7f8c-425d-bdc6-90543a1e15b2",
   "metadata": {},
   "source": [
    "[Search result for our heterophones in the BCCWJ corpus](https://chunagon.ninjal.ac.jp/bccwj-nt/search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf7de05-40fe-47e7-ab3b-ce64561aaf87",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from yomikata.dataset.bccwj import bccwj_data\n",
    "\n",
    "bccwj_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b72bd10-b241-4b2a-ac75-c8296d4592ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "from yomikata import utils\n",
    "\n",
    "input_files = [\n",
    "    Path(config.SENTENCE_DATA_DIR, \"aozora.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"kwdlc.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"bccwj.csv\"),\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"ndlbib.csv\"),\n",
    "]\n",
    "\n",
    "utils.merge_csvs(input_files, Path(config.SENTENCE_DATA_DIR, \"all.csv\"), n_header=1)\n",
    "logger.info(\"âœ… Merged sentence data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5cbdc34-f293-44fe-bfea-9730d17ab1e7",
   "metadata": {},
   "source": [
    "Filter out duplicate sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02935e6c-d2e3-405f-8ccc-01b9f88a8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"all.csv\"))\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sentence'], keep='first')\n",
    "df_no_duplicates.to_csv(Path(config.SENTENCE_DATA_DIR, \"all_filtered.csv\"), index=False)\n",
    "logger.info(\"âœ… Filtered out duplicate sentences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a8101-11a9-4ace-b34d-7e0f100fde68",
   "metadata": {},
   "source": [
    "# Spliting furigana in the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383890c-b721-4cfb-841a-a7f891058f12",
   "metadata": {},
   "source": [
    "First we generate a dictionary of representations of longer furigana in terms of shorter furigana that appear in the corpus. So for example `{å¼•å‡º/ã²ãã }` will be broken down into `{å¼•/ã²}` and `{å‡º/ãã }`. The algorithm attempts to find a set of shorter furigana for which concatenation of surfaces and readings exactly matches the whole or at least the beginning of the longer furigana. It prefers more granular representations (larger amount of shorter furigana) and if two equally granular representations are possible, it picks the one which is composed of furigana with the largest combined frequency in the corpus. It also translates the \"ãƒ¼\" character into specific hiragana representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122bb5f1-afd4-4975-b744-073db6f3c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dataset.breakdown import generate_breakdown_dictionary\n",
    "\n",
    "generate_breakdown_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8301a65-b2d5-40fe-9f8b-d48bca891c6b",
   "metadata": {},
   "source": [
    "Next we use this dictionary to replace all long furigana in the corpus with shorter furigana. By decomposing furigana we get a corpus that allows us to determine readings of kanji surfaces in a more granular way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede6b860-2148-49eb-bc89-f8f7330af283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.dataset import split\n",
    "from yomikata import utils\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "split_dict = utils.load_dict(Path(config.BREAKDOWN_DATA_DIR, \"translations.json\"))\n",
    "logger.info(\"Starting decomposition process, this may take a while...\")\n",
    "split.decompose_furigana(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"all_filtered.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"all_broken_down.csv\"),\n",
    "    split_dict,\n",
    ")\n",
    "logger.info(\"âœ… Decomposed furigana!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c42dde-0741-44cf-97fb-839f23987a4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Making a list of heteronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f391316-42f9-4896-beb9-d731ddb0f952",
   "metadata": {},
   "source": [
    "We use the list from [Sato et al 2022](https://aclanthology.org/2022.lrec-1.770.pdf) as a start. To these we add a list of additional heteronyms picked from the corpus by the frequency of mistakes MeCab tokenizer makes in predicting their readings and arrive at the following list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b5b290-2e1b-409f-a6e2-27d51f07de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heteronyms = \"å›½ç«‹|ä»®å|éºè¨€|å£è…”|ä¸€é€”|æœ€ä¸­|ä¸€è¡Œ|ä¸€å¤œ|ä¸‹é‡Ž|èŠ±å¼|å±±é™°|ä¸Šä¸‹|ä¸–è«–|ç‰§å ´|ä¸€å‘³|æ–½è¡Œ|æ–½å·¥|è»¢ç”Ÿ|æ¸…æµ„|è¿½å¾“|å¢“çŸ³|æ¼¢æ›¸|ä½œæ³•|é»’å­|ç«¶å£²|é–‹çœ¼|æ±‚é“|æ–½æ¥­|å€Ÿå®¶|é¢¨è»Š|èƒŒç­‹|é€†æ‰‹|ç”ŸèŠ±|ä¸€å¯¸|ä¸€åˆ†|ä¸€æ–‡|æ°—éª¨|ç´°ç›®|èˆ¹åº•|ç›¸ä¹—|æ¢…é›¨|é¢¨ç©´|å¤œè©±|é‡Žå…Ž|å†·æ°´|ç¿¡ç¿ |åå…«ç•ª|çŸ³ç¶¿|å…¬æ–‡|èª­æœ¬|å¤æœ¬\"\n",
    "heteronyms = \"å¹´ä¸­|æ°—å‘³|æŸ|å½±éŸ¿|å¤|çˆ¶|ç†|å®å¡š|æˆ‘|ç§|æµ©|ç½®|å½¼|æ‰‹|å³|æ˜“|æŸ±|æ¬¡|æ˜¯|ç™½é«ª|æ–‡å­—|åšå£«|é€ ä½œ|ç›¸|ä»¥|å‰²|å¼•|åº·|åš|å‘Š|ä¸‰é‡|å‰²å½“|å¤ªé¼“|æœŸ|è¿‘å¹³|çœ¼|è¨±|å¼¥|ç´ |å‚|å®‰|ç¶´|è¡€|é¡Ž|çŒ®|æ‰‹ä½œ|æ±Ÿ|ç•‘|ä¸ƒå…µè¡›|é‡Ž|å£|è±Š|å¼˜æ³•|ç”ŸèŠ±|é¡Žéª¨|ç¹”|æ¢…|åŒ—|å¼˜|å¯¾|å®´|æ²¢|æ³•è¡£|çª|ç²—|å¥‡æ€ª|å¿ƒè¡€|é‡Žå…Ž|é£›é¨¨|å¡µ|å ±|èº«ä½“|å³¶æ´¥|èˆ¹|æ€|å¹¸|å…ˆåˆ»|ä¿º|æ ¹|ç‰›|æ–°|åŠ›|ç¨²|æ¼¢æ›¸|æŸ“|ç·’|ç›®|æ˜¥|ä¸Ž|ç²’|è‰å­|ç¤¼æ‹|é»„|èˆžé¶´|é›»ç¯|å¨˜|æ‰€|é“æ¨™|æŽŒ|ç™½|éƒ½|è²å¡š|å²©|åšæ–‡|ç¼¶è©°|é€ |äº¤æ›|ä¸ƒå|èž|èœ‚|æž•|å‹|å­|é‹¼|èŠ±|ç¶š|åŒ–|è£œç¶´|æ·±|è¡£|æ‰‹è¡“|å¿|èƒŽ|æ­¯|é«˜å±±|ç¥ž|å‰|é¦¬éˆ´è–¯|åœŸå™¨|è·å½¹|æ˜¨å¤œ|æ­¢|å»¶|ç´°|å—|é¼»è…”|å¦¾|æ€§éª¨|å¦‚ä½•|å…µæ³•|è„±æ°´|å±±|ç®±|å¸–|é›¨|å…±|ä¸è¶³|é¢¨å‘‚|é™¸å¥¥|èˆˆ|è¦ª|ä¸Šé‡Ž|å»»|æžœ|å­¦|ä¸Šæ‰‹|é¼»|é‡|ä¸¡çœ¼|è—|æ³•å¸«|åºœ|å‡º|å…±å­˜|å³¶|å·|å¤‰æ›|æ²³|é³¥|å°¼|ä¸€å¤œ|ç‰ˆ|æ¸…æµ„|ä½|æ—¥æš®|ä¹å|é¢¨åœŸ|è¨´|ä»Š|é›¨æ°´|ç™½è¡€|ç–±ç˜¡|åœ°|é€Ÿ|é»’|æ–‡|ç¾¤|ç«¹|å½©|ç›´|å°|æ©‹|å³è¡›é–€|è›™|å‡¦|ä¾‹|å®¿|è·¡|æ¶™|æ³•è¯|æ¯…|å…ˆ|å¥¥|æ˜Ÿ|è¿½å¾“|åŸŽ|åµ|é‰„|è¯|å¤ªå¤«|æŽ›|æŸ³|ä¸¡å´|åˆ|æš¦|ä¸|èº¯|å¿ƒ|å¾¡|å°¾|æ ¼å­|æ³Š|ä»Šæ˜”|æ½®|æŸ³ç”°|è…”|å¸‚å ´|å¤œä¸­|ä¸‹|è£•|å¡©|ä½•åˆ†|åˆæˆ¦|å»º|æ²³å²¸|èº«|æµ…|å®¶|æ¹–|å¤«|å¤©|çŽ‰æ‰‹|å‘|è–„|éƒ¡|ç’°|è©±|ç”Ÿå‘½|æ¿|ç¬‘|æ‡|è—»|æ¹¯|è»Š|ç“¦|è€ƒ|é“|å‡½|è€å­|ç´°ã€…|ä¸€æœˆ|ä¸€æ—¥|éƒ·|é“ç¨‹|ç´°å·¥|æ–‡ç§‘|æ­¤|è„†|å‹¢|åœ’|å«|çœ¼é¡|å®ˆ|å¦–|è¼ª|å©¦|å½±|ä¹³æˆ¿|æ´‹|è‰¶|çµ‚|ä½œ|æ³°|çŠ¬|å…‰|æ±|èŠ½è…«|è¦‹ç‰©|ç«¯|è„‚|é‰¤|ä¸‰|åˆ»|ä»|è‰²|èˆ¹åº•|å¤–|å’½å–‰|ä¸‹é¡Ž|ç‰¹é›†|ç”°|æ§‹|å«Œæ°—|é¡”|å®š|é•·|å­¦æ ¡|ä¸¦|è‚|è‰¯|å–|ä½å±…|èˆŸ|æž—|ç›¸æ’²|é€²|å…­|é£›æ²«|è¦‹|åŒäºº|è½|ç¶´æ–¹|å¤œ|è² |æ³•|ç¹°|åˆƒ|ç‰‡|æ•°|å±…|å·±|ä¸€æ¯|æ˜Ž|ä¼´|å¯Œ|æ°—è³ª|æœ€ä¸­|ä¿¡å¤«|æº€|æ–¼|æ°—|å¸‚|æˆ¸|æ—¬|åˆ‘äº‹|èƒ½ç™»|ä¸€å£°|æ®º|æŠ˜|äº”å|æ•|ä¸€å¯¸|å|å…¬æ–‡|éš†|åˆ¤ä¾‹|ç„¼|æ•…éƒ·|é’|ç«‹|ç¥­|ç¶±|å°å±‹|æ²³å£|å—|ç¾Ž|å·¥å­¦|é™½å­|å®®|åƒé‡Œ|åˆ¥|å¹´ä¸Š|ä½“|åŒ»å­¦|å­˜|æ˜Žæ—¥|è‘‰|ç²‰|æŸ„|é ¸|å®—|æ¡‚|ç°|ä½¿|æ—¥ä¸­|æ­©|ç§‘å­¦|è–äºº|å¤§æ‰‹|æž|åˆ¤|å¹|æ–½å·¥|å¼·|å®¿ä¸»|æ³•å­¦|å¥½|è†|ä»‹|æ­©å…µ|ä¸€æ˜¨å¹´|å¼·åŠ›|æ—©|ä¸‰å›½|å…«å¹¡|å°º|å®šå®¶|å‰|é¶|ä¸€å‘³|é…”|åˆ†åˆ¥|é›£æ²»|æ°·|å°å­¦|å‰é§†|è£‚|é¢¨|åŠæœˆ|åˆ†é–“|éº—|è† |ç«œé¦¬|ç´°ç›®|å…¥|æ „|å…|ç²‹|å…µè¡›|ç¯€|ç®¡|é¢¨è»Š|é ­æ•°|é›²|éœ²|è¦‹é€|æœˆ|é€†æ‰‹|é¦™|æŒ¯|å±±åŸŽ|é›ª|èŠ±å¼|ä¸­|æ²¹|å…„|é›‘|çµŒç·¯|å¤æœ¬|åˆæ³•|å¤§è”µ|ç§‹|æ°|æ—¥å‘|ä¸‹æ‰‹|è¨Ž|ä¸­é–“|ä¸ƒ|å“²|é­‚|è¡¨|äº‹|èª |æ—¥æœ¬|ç±³|å•å±‹|ä¸Š|æ¸…æ°´|é«˜é‡Ž|ç‰§|ç‰©|å£|åœŸ|è‡³|æ­£|èŠ½|å¯›|å­«|çŸ³|åŠ©|æ‹|ç­‰|å¢“çŸ³|æµ|è¶Š|æ¡|é‡|æ•¬|ä½•|èµ·|æ™‚è¨ˆ|å‘½|éš›|æµ·|åŒ–å­¦|å¤ª|é…’|åºŠ|ç´|ç”˜è—·|æž¯|å£°|ç‚Ž|å±±æ²³|å™¨|é€£ä¸­|çš®|ä¸€|æ§˜|ä¼|ç´€|é–‹çœ¼|å®|éŠ€æ|å‹|å¢ƒ|ç ‚|å¤§å±±|æ€§|è™«|å´|æœ‰|éª¨|æ­Œ|å®¤|æ™‚|è€³|é§¿æ²³|é–“|åŒ—æ–¹|çŽ©å…·|å…ƒ|äºŒå|ä¸ˆ|ä¸‡|ä¹³|é€|è¡›é–€|ç©‚|æ˜­|é›¶|å“²éƒŽ|èª¿|é¢|åº•åŠ›|é€š|å–„|é–‹ç™º|ä¼šæ´¥|æ°´é¢|ç¡å­|æ˜¨æ—¥|ç·‘è‰²|å©†|ç››åœŸ|è¨€|åˆ|ç·¨|å¢¨|æ¼å ´|é™°|æº|å’³|ç¸|ä¸€è¡Œ|è‹±æ–‡|æ˜Žæ¸…|äºŒé‡|ç€|æ¥|ç­†|å€Ÿå®¶|ä¿¡|å¼µ|ä¸€æ™‚|èª°|ç•°|é™|ä¾å­˜|è¡€ç—‡|æœ«|æ³•å…¸|å²³|å½“|é›»å ´|æ¢…é›¨|æŽ¢|æ‰“|å¢³|ç›¸ä¹—|ç¿¡ç¿ |æœ›|ä¸Šé¡Ž|é­š|è·|èªž|æŠ±|é¦³|æ¥µ|æ¸…|å·Œ|è–|æŠ€|æ£®|ä¾|çƒ|å¥³|ç¾½|åŠ|æ•™|è–è’²|å¾¹|ä¸Šæ–¹|å¾€|å½¦|ç·‘|å€™|ä¸‰è§’|å›º|å¹¼|ä»|åŠ|ä¸‹é‡Ž|å®…|æ­¦|éºè¨€|ä¹|å¤§å‹¢|ç¦å³¶|ç¿¼|é»’å­|å¾©|ç·‘åŒ–|æ‰‹ç¶š|å­|æ°‘|è¼|èµ¤è¡€|ç—…|ä¿‚|åˆ†|ä¸€æ˜¨æ—¥|æ¯|å†…|å ±å‘Š|æš®|äºº|ä¸–|é¬¼|æ±º|å¤§å’Œ|çœŸ|ä¹…|å‹‡|å…µé¦¬|ä»–|å°åˆ¤|åº¦|å ¤|åŽš|å«‚|ä»Šæ—¥|ç™»|å°å…­|å¤|ç¨®|æ˜Žä»£|å·£|å…¶|ç«|ä¸€è¨€|å®|å¹´|çš†|å›|å‰›|é›…|èŠ±å´—|å¤‰åŒ–|å¾å¦»|èµ¤|è¢‹|é‡Œ|ä½™|æ¸¯|æ·³|ç…å­|å‘‰|å†·æ°´|æ‰€è¬‚|é‘‘|é‡‘|é‹¼æ¿|ç™ºè¶³|å¸¸|è»¢ç”Ÿ|è‰|ç–¾é¢¨|è¾º|æ± |å¢“|å·»|ç¶¿|å°å½¢|è§’|æ ¼ä»˜|åå…«ç•ª|æ²»|ç³¸|å¸ƒ|è¡—|è¦³|ç´™|æ°´|æµ|æ„›|å‚|æœ|è²«|ç„¡|éƒ¨å±‹|æ‘|æ—¥|å›½ç«‹|å¤ä»Š|æ¡œ|é»„è‰²|ä¿®|å°|å¾Œ|é¡|é…’é¡ž|æŒ‡|ç©º|æ³‰|ç‹¼|è¦|è²|å››å|ä»”|è–¬|åºƒ|éš å²|èƒŒ|å››|ç ”ç©¶|ä¸€é€”|çŽ‰|ç«¥|æ­¦è”µ|çŸ³å·»|åˆ€|é ­è“‹|éŸ³|å™º|æœ¬|æ‹å­|å…¬|å¯º|å‹•åŠ›|é¡žèš|æ®¿|é¤¨|è¶³è·¡|é¼|è…¹|ç”»|é”|åŒ¹|æ›¸|æ¯›|é§•|å‡ºå±•|å½|ä¸Šä¸‹|ç‚º|å®Ÿ|ç”·|ç‡ƒ|å ´|æ•™åŒ–|å§‰|æ­ª|é¡|èƒ¸|å°|é™„|åƒ|çœ¸|å¯’æ°—|è¥¿éƒ·|å¸|è“å­|ç¨‹|æ°—éª¨|ä¸–è«–|æœ«æœŸ|äººå¦»|è°·é–“|è‰ç´™|å¯¿|æ­³|åŸº|å¤§ç¤¾|æ¨ª|çµ„|å±±æ‘|ç¯|æœ¬æ›¸|å¿—|æ‚ª|ä¼¸å­|æ±‚é“|åº•|å¿ƒè‚º|é«˜|è”µ|æˆ¦|å¤§äºº|ä¼š|é¦¬|è½è‘‰|é›„|é ƒ|è¨³|ç«¶å£²|äººæ°—|èŒ‚|äºŒäºº|ç”º|æ‚²|åŽŸ|ä¹‹|å¹³|ä¿®æ¥­|å¤§åˆ†|ç§˜|å²å­¦|æœ¨|æ¯|ä½|åŸŽè·¡|ä»®å|å¤«å©¦|æŠœ|å•é¡Œ|äºŒ|å³°|ä¸»|å­è¦|ç´…è‘‰|å½¼æ–¹|ç©ºåŠ›|è¡Œ|ç™½çŸ³|ç†±åŠ›|è²§|ä»˜|å‹•å­¦|ä¸€æ–‡|æ˜Žå¾Œæ—¥|æ‰‹æŒ‡|å› |æ‰‹å¡š|è€…|é¢¨ç©´|å¹³é‡Ž|æµ®|å­”|è­œ|å¤§äº‹|ä¹¾|æ¥½|å¥´|ç•™|å‰µ|é™½|å±±é™°|ç”Ÿ|èƒŽä»”|å›½|ä¸‰åƒ|ç´…|ç‹¬|è·¯|è¶³|å€‰|å“|èª­|å¾|åŒ…|ç§¦|æ²¼æ´¥|å‹•å‘|å¾’ç„¶è‰|æ–¹|æ „ä¸‰éƒŽ|å‹•é™|çµŒ|è–å¾³|æ—¥é–“|æ–½æ¥­|ä¿|ç™º|ç­‹|æˆ¿|è£|é ­|æ²¢åºµ|å¢—|éŠ­|èŠ³|å¤œè©±|å¦‚|æ ¹æœ¬|å£è…”|åˆ©ç›Š|åº—|ç¶²|åš¥ä¸‹|å¦»|ç™¾|æ´»|æ¨©|æœ­|ä½•æ™‚|ç¾ä¸–|èª­æœ¬|åž‹|å¤§å®¶|å|ä»£|è°·|æ–‡æ›¸|éº»|æ¥­|å½¢|ä½œæ³•|å¾—|ç”ºå®¶|è²´å¥³|é™°é™½|æœ¨è³ª|èŒ¶é“|è±š|èš•|å¸¯|åƒ|ä¸€æ–¹|å†¬|æµªæ¼«|é‚¦|æ³¢|å¿ƒä¸­|å‘³|ä¾¿|é«˜æ‘|ç‰§å ´|è©©|åˆ‡|æ´²|çŸ³ç¶¿|å¤¢|ä¿Š|ç‡•|å¹»|æ£Ÿ|æ•·|æ¢|ç”Ÿç‰©|æ ¹æ²»|é‡‘è‰²|èƒŒç­‹|å¤§|å¡š|é›·|é–¢|æ®‹å­˜|ç«œ|ç†±|æ¨¹|ç¿|å† |æ–½è¡Œ|é˜²éŒ†|ä¸€ç›®|æ§|å·¦|å…«|å•|è¥¿|ä¸|å¤§è°·|å°å€‰|è‰åœ°|ç¬ |ç­”|æ–‡å­¦|ä¸€åˆ†|æ’­\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ef3ba-1701-4241-99e3-3b41d7c0e220",
   "metadata": {},
   "source": [
    "We look in our sentence data for these known heteronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef2ec62-9bc3-45e2-bee7-554e6da0ab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "full_df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"all_broken_down.csv\"))\n",
    "len(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e25f447-3ac9-4e59-b29e-45d5b284d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = full_df[\n",
    "    full_df[\"sentence\"].str.contains(heteronyms)\n",
    "]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7867b-f5d9-44c4-ad04-51bbeb8c3dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yomikata import utils\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "import random\n",
    "\n",
    "heteronym_dict = {}\n",
    "dictionary_df = pd.read_csv(Path(config.READING_DATA_DIR, \"all.csv\"))\n",
    "dictionary_set = set(dictionary_df.itertuples(index=False, name=None))\n",
    "\n",
    "# for heteronym in [\"æœ‰\"]:\n",
    "for heteronym in heteronyms.split(\"|\"):\n",
    "    furis = df.loc[df[\"sentence\"].str.contains(heteronym), \"furigana\"].values\n",
    "    readings = []\n",
    "    for furi in furis:\n",
    "        reading_list = utils.get_all_surface_readings(heteronym, furi)\n",
    "        readings += reading_list\n",
    "        # readings += [string for string in reading_list if \"ãƒ¼\" not in string and (heteronym, string) in dictionary_set]\n",
    "    ms = Counter(readings)\n",
    "    ms = {k: v for k, v in sorted(ms.items(), key=lambda item: item[1], reverse=True)}\n",
    "    print(heteronym)\n",
    "    print(ms)\n",
    "    heteronym_dict[heteronym] = ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6100d4-ac9f-4404-86d6-7fc2d453652d",
   "metadata": {},
   "source": [
    "We give up on identifying readings for which we have less than 40 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df3667-a666-4cbe-b12b-a7b743352588",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ncut = 40\n",
    "heteronym_dict_cut = {\n",
    "    k: {k2: v2 for (k2, v2) in v.items() if v2 > ncut}\n",
    "    for (k, v) in heteronym_dict.items()\n",
    "}\n",
    "heteronym_dict_cut = {k: v for (k, v) in heteronym_dict_cut.items() if len(v) > 1}\n",
    "print(len(heteronym_dict_cut))\n",
    "heteronym_dict_cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df04397-e376-4ad3-8356-60f527caff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_dict(heteronym_dict_cut, Path(config.CONFIG_DIR, \"heteronyms.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a414fbb7-3584-4e6d-948e-c13cb326b8b5",
   "metadata": {},
   "source": [
    "# Prepare augmented dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9157613e-54ae-4b9f-908e-4b08fc09b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "from yomikata import utils\n",
    "\n",
    "input_files = [\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"aozora.csv\"),\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"kwdlc.csv\"),\n",
    "    # Path(config.SENTENCE_DATA_DIR, \"bccwj.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"ndlbib.csv\"),\n",
    "]\n",
    "\n",
    "utils.merge_csvs(input_files, Path(config.SENTENCE_DATA_DIR, \"augmentation.csv\"), n_header=1)\n",
    "logger.info(\"âœ… Merged sentence data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc14d4e-a1d6-459d-aca3-6e931352c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"augmentation.csv\"))\n",
    "df_no_duplicates = df.drop_duplicates(subset=['sentence'], keep='first')\n",
    "df_no_duplicates.to_csv(Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered.csv\"), index=False)\n",
    "logger.info(\"âœ… Filtered out duplicate sentences!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db85d985-5d1b-40d9-be9c-504405b66da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "augmentation_filtered_path = Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered.csv\")\n",
    "test_optimized_path = Path(config.SENTENCE_DATA_DIR, \"test/test_optimized_strict_heteronyms.csv\")\n",
    "augmentation_filtered_cleaned_path = Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned.csv\")\n",
    "\n",
    "df_augmented = pd.read_csv(augmentation_filtered_path)\n",
    "df_test_optimized = pd.read_csv(test_optimized_path)\n",
    "\n",
    "initial_row_count = len(df_augmented)\n",
    "\n",
    "df_cleaned = df_augmented[~df_augmented['sentence'].isin(df_test_optimized['sentence'])]\n",
    "\n",
    "final_row_count = len(df_cleaned)\n",
    "\n",
    "logger.info(f\"ðŸ”¢ Number of rows removed: {initial_row_count - final_row_count}\")\n",
    "\n",
    "df_cleaned.to_csv(augmentation_filtered_cleaned_path, index=False)\n",
    "\n",
    "logger.info(\"âœ… Cleaned augmentation_filtered.csv and saved to augmentation_filtered_cleaned.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe970ee-a927-44e1-b4b9-9a63676e7dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dataset.split import (\n",
    "    check_data,\n",
    "    filter_dictionary,\n",
    "    filter_simple,\n",
    "    optimize_furigana,\n",
    "    remove_other_readings,\n",
    "    split_data,\n",
    ")\n",
    "\n",
    "logger.info(\"Rough filtering for sentences with heteronyms\")\n",
    "filter_simple(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms.csv\"),\n",
    "    config.HETERONYMS.keys(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb6f714-27af-4280-866d-24a2c1751c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.dataset import split\n",
    "from yomikata import utils\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "split_dict = utils.load_dict(Path(config.BREAKDOWN_DATA_DIR, \"translations.json\"))\n",
    "logger.info(\"Starting decomposition process, this may take a while...\")\n",
    "split.decompose_furigana(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down.csv\"),\n",
    "    split_dict,\n",
    ")\n",
    "logger.info(\"âœ… Decomposed furigana!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354b8d0-e834-4096-aae2-a91e26d5535c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dataset.split import (\n",
    "    check_data,\n",
    "    filter_dictionary,\n",
    "    filter_simple,\n",
    "    optimize_furigana,\n",
    "    remove_other_readings,\n",
    "    split_data,\n",
    ")\n",
    "logger.info(\"Removing heteronyms with unexpected readings\")\n",
    "remove_other_readings(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down_strict.csv\"),\n",
    "    config.HETERONYMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a9b22-cf05-411d-84d4-c80617aa2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from yomikata.config import config, logger\n",
    "from yomikata import utils\n",
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"augmentation_filtered_cleaned_have_heteronyms_broken_down_strict.csv\"))\n",
    "df1 = df1[['sentence', 'furigana']]\n",
    "temp_file = Path(config.SENTENCE_DATA_DIR, \"temp_filtered.csv\")\n",
    "df1.to_csv(temp_file, index=False)\n",
    "\n",
    "input_files = [\n",
    "    temp_file,\n",
    "    Path(config.SENTENCE_DATA_DIR, \"train/train_optimized_strict_heteronyms.csv\"),\n",
    "]\n",
    "utils.merge_csvs(input_files, Path(config.SENTENCE_DATA_DIR, \"train/train_optimized_strict_heteronyms_augmented.csv\"), n_header=1)\n",
    "\n",
    "temp_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81b298-b8c6-4614-8bc4-7b4993dddf29",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb97b00-1bef-4ab4-9fae-b7d23a6f5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dataset.split import (\n",
    "    check_data,\n",
    "    filter_dictionary,\n",
    "    filter_simple,\n",
    "    optimize_furigana,\n",
    "    remove_other_readings,\n",
    "    split_data,\n",
    ")\n",
    "from yomikata.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64f3e4-ec6e-42fa-b67b-b89482dda19b",
   "metadata": {},
   "source": [
    "We extract from the dataset the sentences which include our heteronyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdc7ee9-e135-47ec-b3ad-7a1be21789e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Rough filtering for sentences with heteronyms\")\n",
    "filter_simple(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"all_broken_down.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    config.HETERONYMS.keys(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7286c-4ab0-4be8-a15e-9c59362b11f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Use sudachi to filter out heteronyms in known compounds\")\n",
    "filter_dictionary(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    config.HETERONYMS.keys(),\n",
    "    Dictionary(\"sudachi\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7b4476-1b01-43f1-906c-535da48224f7",
   "metadata": {},
   "source": [
    "Finally we remove sentences that only include heteronyms with readings that we are not trying to predict for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2279ac4-187f-49cf-ab66-207d0c8656a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.info(\"Removing heteronyms with unexpected readings\")\n",
    "remove_other_readings(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"have_heteronyms_simple.csv\"),\n",
    "    Path(config.SENTENCE_DATA_DIR, \"optimized_strict_heteronyms.csv\"),\n",
    "    config.HETERONYMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85b6f02-49f3-4b31-b952-2fcecc352bbe",
   "metadata": {},
   "source": [
    "After checking our data makes sense we do a train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a97b3c-ffed-4f2e-942a-6207b753e248",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = check_data(\n",
    "    Path(config.SENTENCE_DATA_DIR, \"optimized_strict_heteronyms.csv\")\n",
    ")\n",
    "logger.info(\"Performing train/test/split\")\n",
    "split_data(Path(config.SENTENCE_DATA_DIR, \"optimized_strict_heteronyms.csv\"))\n",
    "\n",
    "logger.info(\"Data splits successfully generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ae3b3-7ead-4311-9976-0b4ee4a3e983",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89903d0d-755a-4f15-a833-a099301f5426",
   "metadata": {},
   "source": [
    "We train a BERT classifier model to disambiguate the heteronyms in our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5354776-36a3-4069-8beb-3be415f3eefe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68218fd1-df51-4865-a5b1-5528f25eabd7",
   "metadata": {},
   "source": [
    "Before we start training we do some simple tests using the BERT tokenizer on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff91833d-cb8c-4a53-96bb-147c1f16b29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": str(Path(config.TRAIN_DATA_DIR, \"train_optimized_strict_heteronyms.csv\")),\n",
    "        \"val\": str(Path(config.VAL_DATA_DIR, \"val_optimized_strict_heteronyms.csv\")),\n",
    "        \"test\": str(Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")),\n",
    "    },\n",
    ")\n",
    "from yomikata.dbert import dBert\n",
    "\n",
    "reader = dBert()\n",
    "\n",
    "dataset = dataset.map(\n",
    "    reader.batch_preprocess_function, batched=True, fn_kwargs={\"pad\": False}\n",
    ")\n",
    "dataset = dataset.filter(\n",
    "    lambda entry: any(label != -100 for label in entry[\"labels\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0555463-b3de-47f7-8fd7-f4608eae1426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "labels = []\n",
    "for key in dataset.keys():\n",
    "    print(f\"{key} dataset has {len(dataset[key])} members\")\n",
    "    have_labels = [i for i in dataset[key] if np.max(i[\"labels\"]) != -100]\n",
    "    print(f\"{len(have_labels)} actually contain heteronyms\")\n",
    "    key_length = len(dataset[key])\n",
    "    for i in tqdm(range(key_length), desc=\"Counting labels\"):\n",
    "        labels += [value for value in dataset[key][i][\"labels\"] if value != -100]\n",
    "    print(\"--\")\n",
    "\n",
    "label_counter = Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e20814-91ec-4b82-a423-ed0fa9d0a0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "heteronyms = defaultdict(dict)\n",
    "\n",
    "for label in label_counter:\n",
    "    label_class = reader.label_encoder.index_to_class[label]\n",
    "    (surface, reading) = label_class.split(\":\")\n",
    "    heteronyms[surface][reading] = label_counter[label]\n",
    "\n",
    "for heteronym in reader.heteronyms:\n",
    "    print(\"heteronym:\", heteronym)\n",
    "    total = 0\n",
    "    for reading in heteronyms[heteronym]:\n",
    "        print(reading, heteronyms[heteronym][reading])\n",
    "        total += heteronyms[heteronym][reading]\n",
    "    print(\"total:\", total)\n",
    "    print(\"------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa92840-6476-4da1-9a2d-e92a09de33d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccac467-2d55-43a1-bbb4-799ca1c8f4ed",
   "metadata": {},
   "source": [
    "To train the model in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc67c4b-6346-434b-9ccd-ec50d4db438c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yomikata.dbert import dBert\n",
    "from datasets import load_dataset\n",
    "from yomikata.config import config, logger\n",
    "from pathlib import Path\n",
    "\n",
    "reader = dBert(reinitialize=True)\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": str(Path(config.TRAIN_DATA_DIR, \"train_optimized_strict_heteronyms.csv\")),\n",
    "        \"val\": str(Path(config.VAL_DATA_DIR, \"val_optimized_strict_heteronyms.csv\")),\n",
    "        \"test\": str(Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")),\n",
    "    },\n",
    ")\n",
    "\n",
    "reader.train(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38438d88-e9fd-453b-b6fd-40368b60cc59",
   "metadata": {},
   "source": [
    "Or using to get MLflow integration, experiment tracking, metrics, run the following in command line:\n",
    "\n",
    "```\n",
    "source yomikata/venv/bin/activate\n",
    "\n",
    "python yomikata/yomikata/main.py yomikata/config/dbert-train-args.json\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40615f12-cd82-461c-bf5f-b0992648a43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa9054-8dfd-4c17-97b9-c5aca73e7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from yomikata.config import config, logger\n",
    "from yomikata.dbert import dBert\n",
    "# from yomikata.main import get_artifacts_dir_from_run\n",
    "\n",
    "# artifacts_dir = get_artifacts_dir_from_run(\"e392694b345e4ca19fd97f6a872ced98\")\n",
    "# reader = dBert(artifacts_dir)\n",
    "reader = dBert()\n",
    "\n",
    "from yomikata.dictionary import Dictionary\n",
    "\n",
    "dictreader = Dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1087023-f28c-4960-b12c-114b81cc9a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"çŸ¥ã£ã¦å‚™ãˆã‚‹æ–°åž‹ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚¶è·å ´ãƒ»å®¶åº­ã§ä»Šæ—¥ã‹ã‚‰ã™ã¹ãã“ã¨\"  # çŸ¥[ã—]ã£ã¦å‚™[ããª]ãˆã‚‹æ–°åž‹[ã—ã‚“ãŒãŸ]ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚¶è·å ´[ã—ã‚‡ãã°]ãƒ»å®¶åº­[ã‹ã¦ã„]ã§ä»Šæ—¥[ãã‚‡ã†]ã‹ã‚‰ã™ã¹ãã“ã¨\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f55418-8370-4fb0-87dd-b09b5d32b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"èº«ä½“--æˆ‘ã€…è‡ªèº«ãŒãã‚Œã§ã‚ã‚‹ã¨ã“ã‚ã®è‡ªç„¶\"  # èº«ä½“[ã—ã‚“ãŸã„]--æˆ‘ã€…[ã‚ã‚Œã‚ã‚Œ]è‡ªèº«[ã˜ã—ã‚“]ãŒãã‚Œã§ã‚ã‚‹ã¨ã“ã‚ã®è‡ªç„¶[ã—ãœã‚“]\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba6ba1-459a-4a47-992f-ce79a2735252",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"æ°—ãŒã¤ã„ãŸã‚‚ã®ã‹ãã‚Œã¨ã‚‚å¶ç„¶ã‹ã‚‰ã‹ã€ç‹™ã‚ã‚ŒãŸå›£ä¸ƒãŒãµã¨é¦–ã‚’ã™ãã‚ãŸã®ã§ã€å±ã†ãé‰„æ‰‡ãŒãã®èº«ä½“ã®ä¸Šã‚’é€šã‚Šè¶Šã—ãªãŒã‚‰ã€ä¸åº¦ä¸¦è¡Œã—ã¦å¤§åªæµã®ç§˜è¡“ã‚’ã¤ãã—ã¤ã¤ã‚ã£ãŸå³å´å‘ã†ã®ã€é»’ä½å›£ä¸ƒãªã‚‰ã¬å¤é«˜æ–°å…µè¡›ã®è„‡è…¹ã«ã€ã¯ãƒƒã—ã¨å‘½ä¸­ã„ãŸã—ã¾ã—ãŸã€‚\"  # ,æ°—[ã]ãŒã¤ã„ãŸã‚‚ã®ã‹ãã‚Œã¨ã‚‚å¶ç„¶[ãã†ãœã‚“]ã‹ã‚‰ã‹ã€ç‹™[ã­ã‚‰]ã‚ã‚ŒãŸå›£ä¸ƒ[ã ã‚“ã—ã¡]ãŒãµã¨é¦–[ãã³]ã‚’ã™ãã‚ãŸã®ã§ã€å±[ã‚ã‚„]ã†ãé‰„æ‰‡[ã¦ã£ã›ã‚“]ãŒãã®èº«ä½“[ã‹ã‚‰ã ]ã®ä¸Š[ã†ãˆ]ã‚’é€š[ã¨ãŠ]ã‚Šè¶Š[ã“]ã—ãªãŒã‚‰ã€ä¸åº¦[ã¡ã‚‡ã†ã©]ä¸¦è¡Œ[ã¸ã„ã“ã†]ã—ã¦å¤§åªæµ[ãŠãŠã¤ã¼ã‚Šã‚…ã†]ã®ç§˜è¡“[ã²ã˜ã‚…ã¤]ã‚’ã¤ãã—ã¤ã¤ã‚ã£ãŸå³å´[ã¿ãŽãŒã‚]å‘[ã‚€ã“]ã†ã®ã€é»’ä½[ãã‚ãšã¿]å›£ä¸ƒ[ã ã‚“ã—ã¡]ãªã‚‰ã¬å¤é«˜[ãµã‚‹ãŸã‹]æ–°å…µè¡›[ã—ã‚“ã¹ãˆ]ã®è„‡è…¹[ã‚ãã°ã‚‰]ã«ã€ã¯ãƒƒã—ã¨å‘½ä¸­[ã‚ã„ã¡ã‚…ã†]ã„ãŸã—ã¾ã—ãŸã€‚\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a555bad-c5ed-484d-b3b5-fa2f583d5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Interviewé›²ç”°ã¯ã‚‹ã“:BLã‹ã‚‰ã€Žæ˜­å’Œå…ƒç¦„è½èªžå¿ƒä¸­ã€ã¾ã§äººé–“ã®å€‹æ€§ã‚’è¦‹ã¤ã‚ã‚‹ç¨€ä»£ã®æãæ‰‹\"  # ,Interviewé›²ç”°[ã†ã‚“ã§ã‚“]ã¯ã‚‹ã“:BLã‹ã‚‰ã€Žæ˜­å’Œ[ã—ã‚‡ã†ã‚]å…ƒç¦„[ã’ã‚“ã‚ã]è½èªž[ã‚‰ãã”]å¿ƒä¸­[ã—ã‚“ã˜ã‚…ã†]ã€ã¾ã§äººé–“[ã«ã‚“ã’ã‚“]ã®å€‹æ€§[ã“ã›ã„]ã‚’è¦‹[ã¿]ã¤ã‚ã‚‹ç¨€ä»£[ããŸã„]ã®æ[ãˆãŒ]ãæ‰‹[ã¦]\n",
    "reader.furigana(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d29a28-06cf-4dfc-a741-89320323f1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ç‰¹é›†ç”Ÿæˆã™ã‚‹èº«ä½“\"  # ,ç‰¹é›†[ã¨ãã—ã‚…ã†]ç”Ÿæˆ[ã›ã„ã›ã„]ã™ã‚‹èº«ä½“[ã—ã‚“ãŸã„]\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a91ce1-4a8b-4447-9fb7-68bff37dfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ä»Šæ—¥ã®ä¸–ç•Œæƒ…å‹¢ã¯\"\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc58803-f63a-4a16-80c1-f0a4294ec195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ã‚ã®åŠ›å£«ã«ã¯é‡‘æ˜Ÿã¯ã©ã‚Œãã‚‰ã„ã‚ã‚‹ï¼Ÿ\"\n",
    "print(dictreader.furigana(reader.furigana(text)))\n",
    "print(dictreader.furigana(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610528e3-11ed-4a67-b757-ad78d13ee6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"é»„è‰²ã¨é»’ã®çµ„ã¿åˆã‚ã›ã¯ã€å±é™ºã§ã‚ã‚‹ã“ã¨ã‚’è¡¨ã™\"\n",
    "reader.furigana(text)  # è¡¨ã€€is in but è¡¨ã™ã€€is properly parsed as not ambiguous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14d57bf-ef34-45f1-9fd8-72ec19a859c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"è¡¨å‚é“ã«è¡Œãã¾ã™\"\n",
    "reader.furigana(\n",
    "    text\n",
    ")  # è¡¨ã€€is in but since is in the compound è¡¨å‚é“ã€€ it is properly recognized as something that should be looked up in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369962d2-9395-461f-bbd9-aa7d7ea1f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ãã®è¡¨ã‚’è¦‹ã›ã¦ãã ã•ã„\"\n",
    "reader.furigana(text)  # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4350f7-e4cb-4d8c-a30f-34a7727e75a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ã‚ã®å®¶ã®è¡¨ã¯ç¶ºéº—ã§ã™\"\n",
    "reader.furigana(text)  # Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c266f685-837a-4b2d-b802-9fb4ae0af93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"å»ºç¯‰è¡¨ã‚’è¦‹ã›ã¦ãã ã•ã„\"\n",
    "reader.furigana(text)  # Failed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60451c5-8c54-4057-a0b4-e2f1fb8547fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Code structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0374ab2d-29d0-4c3e-80de-d7545e3c27aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata import utils\n",
    "from yomikata.dbert import dBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0254f905-cede-4ffd-941a-39bf8c4cefe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = dBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013e51e-9bb7-481c-bc7b-e7f7e6679718",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'ãã—ã¦ã€{ç•³/ãŸãŸã¿}ã®{è¡¨/ãŠã‚‚ã¦}ã¯ã€ã™ã§ã«{å¹¾/ã„ã}{å¹´/ã­ã‚“}{å‰/ã¾ãˆ}ã«{æ›/ã‹}ãˆã‚‰ã‚ŒãŸã®ã‹{åˆ†/ã‚ã‹}ã‚‰ãªã‹ã£ãŸ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8eb00-259f-4c7d-9a21-8cddf11a7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "disambiguated_sentence = reader.furigana(utils.remove_furigana(test_sentence))\n",
    "print(disambiguated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d5009-cc90-45d4-9c6a-59be622ab019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dictionary import Dictionary\n",
    "\n",
    "dictreader = Dictionary()\n",
    "dictreader.furigana(utils.remove_furigana(test_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f302515-926f-40cb-9c97-8fba20dec1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictreader.furigana(disambiguated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd3afd-f4c6-4c89-bc8a-59fe3254ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictreader.furigana(disambiguated_sentence) == dictreader.furigana(\n",
    "    utils.remove_furigana(test_sentence)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7804737b-4dbe-435c-9736-eeca60b40dd0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055569b-07dc-42fc-930f-6ab6e4be5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from yomikata.config import config\n",
    "from yomikata.dbert import dBert\n",
    "from yomikata.main import get_artifacts_dir_from_run\n",
    "\n",
    "# artifacts_dir = get_artifacts_dir_from_run(\"e392694b345e4ca19fd97f6a872ced98\")\n",
    "# artifacts_dir = Path(\n",
    "#    get_artifacts_dir_from_run(\"4d19dfb0d0b64b518d8e5506e3f6a726\"), \"checkpoint-10200\"\n",
    "# )\n",
    "\n",
    "reader = dBert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fc125-0911-4cd1-8b26-97cc09ec5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": str(\n",
    "            Path(config.TRAIN_DATA_DIR, \"train_optimized_strict_heteronyms.csv\")\n",
    "        ),\n",
    "        \"val\": str(Path(config.VAL_DATA_DIR, \"val_optimized_strict_heteronyms.csv\")),\n",
    "        \"test\": str(Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")),\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset = dataset.map(\n",
    "    reader.batch_preprocess_function, batched=True, fn_kwargs={\"pad\": False}\n",
    ")\n",
    "dataset = dataset.filter(\n",
    "    lambda entry: any(label != -100 for label in entry[\"labels\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7017bc-9b98-43ef-96f9-a3011605e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from yomikata.custom_bert import CustomDataCollatorForTokenClassification\n",
    "import evaluate\n",
    "\n",
    "data_collator = CustomDataCollatorForTokenClassification(\n",
    "    tokenizer=reader.tokenizer, padding=True\n",
    ")\n",
    "\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p  # predictions are already the argmax of logits\n",
    "    true_predictions = [pred for prediction, label in zip(predictions, labels) for pred, lab in zip(prediction, label) if lab != -100]\n",
    "    true_labels = [lab for prediction, label in zip(predictions, labels) for pred, lab in zip(prediction, label) if lab != -100]\n",
    "    return {\"accuracy\": accuracy_metric.compute(references=true_labels, predictions=true_predictions)[\"accuracy\"], \"recall\": recall_metric.compute(references=true_labels, predictions=true_predictions, average=\"macro\", zero_division=0)[\"recall\"]}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=reader.model,\n",
    "    tokenizer=reader.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=lambda logits, _: torch.argmax(logits, dim=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd520e0a-0a22-40e6-9678-6dabb9ada96d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from yomikata.config import logger\n",
    "\n",
    "reader.model.eval()\n",
    "full_performance = {}\n",
    "# for key in dataset.keys():\n",
    "for key in [\"test\"]:\n",
    "    max_evals = min(1000000, len(dataset[key]))\n",
    "    # max_evals = len(dataset[key])\n",
    "    logger.info(f\"getting predictions for {key}\")\n",
    "    subset = dataset[key].shuffle().select(range(max_evals))\n",
    "    prediction_output = trainer.predict(subset)\n",
    "    logger.info(f\"processing predictions for {key}\")\n",
    "    metrics = prediction_output[2]\n",
    "    labels = prediction_output[1]\n",
    "\n",
    "    logger.info(\"processing performance\")\n",
    "    performance = {\n",
    "        heteronym: {\n",
    "            \"n\": 0,\n",
    "            \"readings\": {\n",
    "                reading: {\n",
    "                    \"n\": 0,\n",
    "                    \"found\": {readingprime: 0 for readingprime in list(reader.heteronyms[heteronym].keys())}\n",
    "                }\n",
    "                for reading in list(reader.heteronyms[heteronym].keys())\n",
    "            },\n",
    "        }\n",
    "        for heteronym in reader.heteronyms.keys()\n",
    "    }\n",
    "\n",
    "    flattened_logits = [\n",
    "        logit\n",
    "        for sequence_logits, sequence_labels in zip(prediction_output[0], labels)\n",
    "        for (logit, l) in zip(sequence_logits, sequence_labels) if l != -100\n",
    "    ] # this is already argmaxed in preprocess_logits_for_metrics, so the resulting list is 1d. valid_mask processing in CustomBertForTokenClassification.forward takes care of zeoring out irrelevant logits\n",
    "\n",
    "    true_labels = [\n",
    "        str(reader.label_encoder.index_to_class[l])\n",
    "        for label in labels\n",
    "        for l in label if l != -100\n",
    "    ]\n",
    "\n",
    "    for i, true_label in enumerate(true_labels):\n",
    "        (true_surface, true_reading) = true_label.split(\":\")\n",
    "        performance[true_surface][\"n\"] += 1\n",
    "        performance[true_surface][\"readings\"][true_reading][\"n\"] += 1\n",
    "        predicted_label = reader.label_encoder.index_to_class[flattened_logits[i]]\n",
    "        predicted_reading = predicted_label.split(\":\")[1]\n",
    "        performance[true_surface][\"readings\"][true_reading][\"found\"][predicted_reading] += 1\n",
    "\n",
    "    for surface in performance:\n",
    "        for true_reading in performance[surface][\"readings\"]:\n",
    "            true_count = performance[surface][\"readings\"][true_reading][\"n\"]\n",
    "            predicted_count = performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "            performance[surface][\"readings\"][true_reading][\"accuracy\"] = predicted_count / true_count if true_count > 0 else \"NaN\"\n",
    "        correct_count = sum(performance[surface][\"readings\"][true_reading][\"found\"][true_reading] for true_reading in performance[surface][\"readings\"])\n",
    "        all_count = performance[surface][\"n\"]\n",
    "        performance[surface][\"accuracy\"] = correct_count / all_count if all_count > 0 else \"NaN\"\n",
    "\n",
    "    performance = {\n",
    "        \"metrics\": metrics,\n",
    "        \"heteronym_performance\": performance,\n",
    "    }\n",
    "\n",
    "    full_performance[key] = performance\n",
    "\n",
    "full_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1c56c-27f7-4990-86ed-dc9ba4b53be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Performance for dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04578d8-e2f8-42e0-b4f4-167ba8e995df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from yomikata.config import config, logger\n",
    "from speach.ttlig import RubyFrag, RubyToken\n",
    "from yomikata import utils\n",
    "from yomikata.dictionary import Dictionary\n",
    "from yomikata.dataset import breakdown\n",
    "from yomikata.dataset.split import replace_furigana\n",
    "\n",
    "reader = Dictionary(\"sudachi\")\n",
    "heteronyms = config.HETERONYMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04c0d8f-7beb-4999-9f54-12c26a965a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path(config.TEST_DATA_DIR, \"test_optimized_strict_heteronyms.csv\")\n",
    "max_evals = 1000000\n",
    "df = pd.read_csv(filename, header=0)\n",
    "df = df.sample(frac=1)\n",
    "if max_evals is not None:\n",
    "    max_evals = max(max_evals, 1)\n",
    "    max_evals = min(max_evals, len(df))\n",
    "    df = df.head(max_evals)\n",
    "\n",
    "df[\"furigana_found\"] = df.apply(\n",
    "    lambda x: reader.furigana(utils.standardize_text(x[\"sentence\"])), axis=1\n",
    ")\n",
    "\n",
    "sentences = df[\"furigana_found\"].tolist()\n",
    "sentences += df[\"furigana\"].tolist()\n",
    "(split_dict, no_translation) = breakdown.sentence_list_to_breakdown_dictionary(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202971cd-ad3c-450e-bf6c-33406613e17e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"furigana_found\"] = df[\"furigana_found\"].apply(\n",
    "    lambda s: replace_furigana(s, split_dict)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454d861-2578-452c-950f-26091e8bc6a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "performance = {\n",
    "    heteronym: {\n",
    "        \"n\": 0,\n",
    "        \"readings\": {\n",
    "            reading: {\n",
    "                \"n\": 0,\n",
    "                \"found\": {\n",
    "                    readingprime: 0\n",
    "                    for readingprime in list(heteronyms[heteronym].keys()) + [\"<OTHER>\"]\n",
    "                },\n",
    "            }\n",
    "            for reading in list(heteronyms[heteronym].keys())\n",
    "        },\n",
    "    }\n",
    "    for heteronym in heteronyms.keys()\n",
    "}\n",
    "failures = 0\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"processing performance\"):\n",
    "    matches = utils.find_all_substrings(row[\"sentence\"], heteronyms.keys())\n",
    "    furis_true = utils.get_furis(row[\"furigana\"])\n",
    "    furis_found = utils.get_furis(row[\"furigana_found\"])\n",
    "    failure = False\n",
    "    for location in matches:\n",
    "        surface = matches[location]\n",
    "        reading_true = utils.get_reading_from_furi(location, len(surface), furis_true)\n",
    "        if not reading_true:\n",
    "            continue\n",
    "        reading_found = utils.get_reading_from_furi(location, len(surface), furis_found)\n",
    "        if not reading_found:\n",
    "#            print(location, surface, row[\"furigana\"], row[\"furigana_found\"], row[\"sentence\"])\n",
    "            failure = True\n",
    "        performance[surface][\"n\"] += 1\n",
    "        if (reading_true in performance[surface][\"readings\"].keys()):\n",
    "            found_reading = reading_found if reading_found in performance[surface][\"readings\"].keys() else \"<OTHER>\"\n",
    "            performance[surface][\"readings\"][reading_true][\"n\"] += 1\n",
    "            performance[surface][\"readings\"][reading_true][\"found\"][found_reading] += 1\n",
    "    if failure:\n",
    "        failures += 1\n",
    "n = 0\n",
    "correct = 0\n",
    "for surface in performance.keys():\n",
    "    for true_reading in performance[surface][\"readings\"].keys():\n",
    "        performance[surface][\"readings\"][true_reading][\"accuracy\"] = np.round(\n",
    "            performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "            / np.array(performance[surface][\"readings\"][true_reading][\"n\"]),\n",
    "            3,\n",
    "        )\n",
    "\n",
    "    performance[surface][\"accuracy\"] = np.round(\n",
    "        sum(\n",
    "            performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "            for true_reading in performance[surface][\"readings\"].keys()\n",
    "        )\n",
    "        / np.array(performance[surface][\"n\"]),\n",
    "        3,\n",
    "    )\n",
    "\n",
    "    correct += sum(\n",
    "        performance[surface][\"readings\"][true_reading][\"found\"][true_reading]\n",
    "        for true_reading in performance[surface][\"readings\"].keys()\n",
    "    )\n",
    "    n += performance[surface][\"n\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b4f0e8-5fbd-4017-b060-410a5f5245c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(failures, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b9f9e-b859-4144-b8a7-e7178590be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total accuracy:\", correct/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0686759e-bf78-4ea8-a519-41f8fab8b570",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print({key: performance[key][\"accuracy\"] for key in performance.keys()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc6caa-7cdc-4d5f-ac45-16f4bf69ffbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37efa2da-01a3-4207-a9d4-b15af9812303",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Details of classifying based on textual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea2daf-850e-464a-99f1-638e1ee2f29a",
   "metadata": {},
   "source": [
    "With the T5 model I am fine-tuning the whole encoder-decoder architecture to encode the embeddings and then output the correct readings for every token. This assumes essentially that every token can be ambiguous and can have any possible reading.\n",
    "\n",
    "The Amazon paper does something simpler. It takes the BERT encodings as input and for ambiguous tokens trains a small classifier model to choose between 2 or 3 readings. Is this kind of thing possible for Japanese? Let's look at some tokenizations and see if such a thing is possible for japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769be0a1-7362-47ed-8839-737e8fdefe73",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Proof of concept: Do contextual embeddings significantly differ for heteronyms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac06b92-8c5c-47c2-8ab8-75cb1a0af1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"é‡‘æ˜Ÿ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f36b8d9-2807-4d2a-a16d-0ab4527c7786",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"é‡‘æ˜Ÿã¯å¤ªé™½ç³»ã§å¤ªé™½ã«è¿‘ã„æ–¹ã‹ã‚‰2ç•ªç›®ã®æƒ‘æ˜Ÿã€‚\"\n",
    "text2 = \"é‡‘æ˜Ÿã¨ã¯ã€å¤§ç›¸æ’²ã§ã€å¹³å¹•ã®åŠ›å£«ãŒæ¨ªç¶±ã¨å–çµ„ã‚’ã—ã¦å‹åˆ©ã™ã‚‹ã“ã¨ã§ã‚ã‚‹ã€‚\"\n",
    "texts = [text1, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e65a0-8c86-4355-8b26-1f8ecfcb007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dictionary import DictionaryReader\n",
    "\n",
    "DicReader = DictionaryReader()\n",
    "for text in texts:\n",
    "    print(DicReader.tagger(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfc38d4-6661-4d74-867e-d43672cbfe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on tokenizer results below å¹³å¹• appears to be in unidic but not unidic_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f67b2-21bd-439c-bc84-2d62f11ba54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6d66c9-5185-4dae-9129-1a8abd2e1f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "for text in texts:\n",
    "    text_encoded = tokenizer(\n",
    "        text,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    input_ids = text_encoded[\"input_ids\"]\n",
    "    input_mask = text_encoded[\"attention_mask\"]\n",
    "    print(input_ids)\n",
    "    print([tokenizer._convert_id_to_token(input_id) for input_id in input_ids])\n",
    "    tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e1d702-5f4a-4247-a5ea-707a8282d45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d064380-2d09-484e-b1a9-3ba35c9cf179",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in texts:\n",
    "    text_encoded = tokenizer(\n",
    "        text,\n",
    "        max_length=16,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "        add_special_tokens=False,\n",
    "    )  # needs to be pytorch tensors\n",
    "    input_ids = text_encoded[\"input_ids\"]\n",
    "    input_mask = text_encoded[\"attention_mask\"]\n",
    "\n",
    "    print(input_ids.shape)\n",
    "\n",
    "    outputs = model.forward(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "    print(outputs.last_hidden_state)\n",
    "    print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067a8d4-df70-4045-a993-ef5b7fcb78b4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Embedding visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef289a-5389-43e2-9ca3-5a14ca71dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer, BertModel\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model.eval()\n",
    "import numpy as np\n",
    "\n",
    "words = np.array(list(tokenizer.vocab.keys()))\n",
    "wordembs = model.embeddings.word_embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87b7766-a699-4737-b9ac-21fabb8de3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordembs.shape)  # 32768 is the vocab size and 768 the embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a26f5dd-1de0-42ec-81fc-064c45f1678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs = wordembs.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524fbcab-997b-48b9-b0ab-a0421f7c9bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine vocabulary to use for t-SNE/visualization. The indices are hard-coded based partially on inspection:\n",
    "char_indices_to_use = np.arange(851, 1063, 1)\n",
    "voc_indices_to_plot = np.append(char_indices_to_use, np.arange(23000, 27000, 1))\n",
    "voc_indices_to_use = np.append(char_indices_to_use, np.arange(17000, 27000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199accac-416f-48b8-846f-e2cc157a497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(voc_indices_to_plot))\n",
    "print(len(voc_indices_to_use))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56655c71-8241-49ba-b3b2-15b371b2ffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(words[bert_voc_indices_to_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a13f39-291d-4353-b093-3a3b45a9c26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs_to_use = wordembs[voc_indices_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b1710b-bbb0-42e9-a66b-14a8a9d80966",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Run t-SNE on the BERT vocabulary embeddings we selected:\n",
    "mytsne_words = TSNE(n_components=2, early_exaggeration=12, metric=\"cosine\", init=\"pca\")\n",
    "wordembs_to_use_tsne = mytsne_words.fit_transform(wordembs_to_use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc95547c-97bc-4198-844d-ed946e25d4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs_to_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fce6e7-1c62-4ac5-9127-42d63243b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembs_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213726c1-11de-45cb-a3bd-f17e3e5c1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_plot = words[voc_indices_to_plot]\n",
    "print(len(words_to_plot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f1b3b8-7022-4538-90c9-61675bd1dc12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the transformed BERT vocabulary embeddings:\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"VL Gothic\"\n",
    "\n",
    "fig = plt.figure(figsize=(100, 60))\n",
    "alltexts = list()\n",
    "for i, txt in enumerate(words_to_plot):\n",
    "    plt.scatter(wordembs_to_use_tsne[i, 0], wordembs_to_use_tsne[i, 1], s=0)\n",
    "    currtext = plt.text(wordembs_to_use_tsne[i, 0], wordembs_to_use_tsne[i, 1], txt)\n",
    "    alltexts.append(currtext)\n",
    "\n",
    "\n",
    "# Save the plot before adjusting.\n",
    "plt.savefig(\"japanese-viz-bert-voc-noadj.pdf\", format=\"pdf\")\n",
    "# print('now running adjust_text')\n",
    "# Using autoalign often works better in my experience, but it can be very slow for this case, so it's false by default below:\n",
    "# numiters = adjust_text(alltexts, autoalign=True, lim=50)\n",
    "# from adjustText import adjust_text\n",
    "# numiters = adjust_text(alltexts, autoalign=False, lim=50)\n",
    "# print('done adjust text, num iterations: ', numiters)\n",
    "# plt.savefig('japanese-viz-bert-voc-tsne10k-viz4k-adj50.pdf', format='pdf')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23952e8-e189-4048-9414-c2100721897b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### ãƒ•ã‚©ãƒ³ãƒˆä¸€è¦§ã‚’ç¢ºèªã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.font_manager as fm\n",
    "# import numpy as np\n",
    "\n",
    "# fonts = list(np.unique([f.name for f in matplotlib.font_manager.fontManager.ttflist]))\n",
    "\n",
    "# fig = plt.figure(figsize=(8, 100))\n",
    "# ax = fig.add_subplot(1, 1, 1)\n",
    "# ax.set_ylim([-1, len(fonts)])\n",
    "# ax.set_yticks(np.arange(0, len(fonts), 10))\n",
    "\n",
    "# for i, f in enumerate(fonts):\n",
    "#     ax.text(0.2, i,  'æ—¥æœ¬èªžå¼· {}'.format(f), fontdict={'family': f, 'fontsize': 14})\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ee5648-4242-47a8-91b0-c4626a94c891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from yomikata.config import config, logger\n",
    "\n",
    "df = pd.read_csv(Path(config.SENTENCE_DATA_DIR, \"aozora.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ff09c-9c4c-4383-9fd7-aa1c27708fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"å¸‚å ´\"\n",
    "word_classes = [\"ã—ã˜ã‚‡ã†\", \"ã„ã¡ã°\"]\n",
    "word = \"ç¤¼æ‹\"\n",
    "word_classes = [\"ã‚Œã„ã¯ã„\", \"ã‚‰ã„ã¯ã„\"]\n",
    "word = \"ä»Šæ—¥\"\n",
    "word_classes = [\"ãã‚‡ã†\", \"ã“ã‚“ã«ã¡\"]\n",
    "word = \"ä»Šæ—¥\"\n",
    "word_classes = [\"ãã‚‡ã†\", \"ã“ã‚“ã«ã¡\"]\n",
    "word = \"è¡¨\"\n",
    "word_classes = [\"ã²ã‚‡ã†\", \"ãŠã‚‚ã¦\"]\n",
    "word = \"ä»®å\"\n",
    "word_classes = [\"ã‹ãª\", \"ã‹ã‚ã„\"]\n",
    "word = \"å¤‰åŒ–\"\n",
    "word_classes = [\"ã¸ã‚“ã‹\", \"ã¸ã‚“ã’\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d48620-38f1-4441-a791-6b27d2609ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.heteronyms import heteronyms\n",
    "\n",
    "print(heteronyms[heteronyms[\"surface\"] == word])\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "pronunciation_df = pd.read_csv(Path(config.PRONUNCIATION_DATA_DIR, \"all.csv\"))\n",
    "print(pronunciation_df[pronunciation_df[\"surface\"] == word][\"pronunciations\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e53b35-0e8e-400c-9b84-2aae56cc7b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = df[df[\"sentence\"].str.contains(word)]\n",
    "df_keyword = df_keyword.reset_index(drop=True)\n",
    "window_size = 128\n",
    "df_keyword[\"sentence-shorter\"] = df_keyword[\"sentence\"].apply(\n",
    "    lambda sentence: (\n",
    "        idx := sentence.index(word),\n",
    "        sentence[np.max([0, idx - window_size]) : idx]\n",
    "        + sentence[idx : np.min([len(sentence), idx + window_size])],\n",
    "    )[1]\n",
    ")\n",
    "print(len(df_keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58e01a-45a6-42ff-9053-ba9cb0762879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reading_matcher(furigana, word, word_classes):\n",
    "    try:\n",
    "        shifted_furigana = furigana[furigana.index(word) :]\n",
    "    except ValueError:\n",
    "        print(word)\n",
    "        print(furigana)\n",
    "        return -1\n",
    "    found_reading = shifted_furigana[\n",
    "        shifted_furigana.index(\"[\") + 1 : shifted_furigana.index(\"]\")\n",
    "    ]\n",
    "    # print(found_reading)\n",
    "    for reading in word_classes:\n",
    "        if found_reading.find(reading) != -1:\n",
    "            return reading\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d45c3-ba5d-40f2-b122-88f0132c8500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword[\"reading\"] = df_keyword[\"furigana\"].apply(\n",
    "    lambda sentence: reading_matcher(sentence, word, word_classes)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d65ab-952b-4dee-ba61-b82bbf15a42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Improve the code for classifying words with furigana into one of the reading classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7715b1f0-57bf-414f-b59f-2cc25b0598a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word_class in word_classes:\n",
    "    print(f\"{word_class} {len(df_keyword[df_keyword['reading'] == word_class])}\")\n",
    "print(\"failures\", len(df_keyword[df_keyword[\"reading\"] == -1]))\n",
    "df_keyword[df_keyword[\"reading\"] == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d121819-653a-4d9f-b320-6fa6ed7f1984",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_keyword = df_keyword[df_keyword[\"reading\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc679c-d319-4092-8638-57660ecc5171",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id = tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "pad_size = 32\n",
    "df_keyword[\"sentence-encoded\"] = df_keyword[\"sentence-shorter\"].apply(\n",
    "    lambda sentence: tokenizer.encode(\n",
    "        sentence,\n",
    "        add_special_tokens=False,\n",
    "        max_length=pad_size,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    ")\n",
    "df_keyword[\"encoding-success\"] = df_keyword[\"sentence-encoded\"].apply(\n",
    "    lambda encoding: word_id in encoding\n",
    ")\n",
    "print(len(df_keyword[~df_keyword[\"encoding-success\"]]), \"encoding failures\")\n",
    "df_keyword = df_keyword[df_keyword[\"encoding-success\"]]\n",
    "df_keyword = df_keyword.reset_index(drop=True)\n",
    "df_keyword[\"keyword-index\"] = df_keyword[\"sentence-encoded\"].apply(\n",
    "    lambda encoding: encoding.index(word_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359c555d-140d-4bc7-b6c6-994b12993ee8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_keyword[\"keyword-index\"] = df_keyword[\"sentence-encoded\"].apply(\n",
    "    lambda encoding: encoding.index(word_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20889a9-925e-4b2b-bf2c-671c76495ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_stack = np.vstack(df_keyword[\"sentence-encoded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31210184-4fa3-4b86-acf6-5166aa4f5d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "forward_pass = model.forward(torch.tensor(encoding_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d79da28-1f74-4f72-9c5f-c1ff70ff43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(forward_pass[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79983eb-fd3b-4ba7-adb5-f9906577a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "for i in range(len(df_keyword)):\n",
    "    embs.append(forward_pass[0][i][df_keyword.at[i, \"keyword-index\"]].detach().numpy())\n",
    "embs = np.array(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48403c30-8463-4d31-ac7c-aa20c0aec7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Run t-SNE on the contextualized embeddings:\n",
    "mytsne_tokens = TSNE(\n",
    "    n_components=2,\n",
    "    early_exaggeration=12,\n",
    "    verbose=2,\n",
    "    metric=\"cosine\",\n",
    "    init=\"pca\",\n",
    "    n_iter=2000,\n",
    ")\n",
    "embs_tsne = mytsne_tokens.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3777f6-c567-4920-95d2-71ca0fe1d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the keyword+context strings.\n",
    "import japanize_matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"VL Gothic\"\n",
    "\n",
    "colors = [\"red\", \"black\", \"blue\", \"green\"]\n",
    "classes = list(df_keyword[\"reading\"].unique())\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "cs = [\n",
    "    colors[classes.index(df_keyword[\"reading\"].iloc[i])] for i in range(len(df_keyword))\n",
    "]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.scatter(embs_tsne[:, 0], embs_tsne[:, 1], s=1, color=cs)\n",
    "\n",
    "plt.savefig(\"japanese-viz-bert-ctx-points-\" + word + \".pdf\", format=\"pdf\")\n",
    "plt.savefig(\"japanese-viz-bert-ctx-points-\" + word + \".png\", format=\"png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4c3092-8bd7-4460-a735-6e3ea3aed91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the keyword+context strings.\n",
    "# import matplotlib.pyplot as plt\n",
    "# import japanize_matplotlib\n",
    "\n",
    "# plt.rcParams[\"font.family\"] = \"VL Gothic\"\n",
    "\n",
    "# colors = ['red', 'black']\n",
    "# classes = list(df_keyword['reading'].unique())\n",
    "# fig = plt.figure(figsize=(50, 30))\n",
    "# alltexts = list()\n",
    "# for i, txt in enumerate(df_keyword['sentence-shorter']):\n",
    "#     if i % 100 == 0:\n",
    "#         print(i)\n",
    "#     plt.scatter(embs_tsne[i,0], embs_tsne[i,1], s=0)\n",
    "#     c = colors[classes.index(df_keyword['reading'].iloc[i])]\n",
    "#     currtext = plt.text(embs_tsne[i,0], embs_tsne[i,1], txt, color=c)\n",
    "#     #alltexts.append(currtext)\n",
    "\n",
    "# plt.savefig('japanese-viz-bert-ctx-text-'+word+'.pdf', format='pdf')\n",
    "# # print('now running adjust_text')\n",
    "# #numiters = adjust_text(alltexts, autoalign=True, lim=50)\n",
    "# #numiters = adjust_text(alltexts, autoalign=False, lim=50)\n",
    "# #print('done adjust text, num iterations: ', numiters)\n",
    "# #plt.savefig('viz-bert-ctx-values-viz750-adj.pdf', format='pdf')\n",
    "\n",
    "# plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6255d6-1c9c-40c0-92a3-3bdcf7815a88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Handling out of vocab heteronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403dbad-506b-475c-9723-81997a12bc51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"ãã®åŠ›å£«ã«ã¯é‡‘æ˜ŸãŒå¤šãã¦å¤§äººæ°—ã€‚\"\n",
    "text = \"ä¸€æ™‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c944f-ae1d-48c1-8e8c-8e8e81c51116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yomikata.dictionary import DictionaryReader\n",
    "\n",
    "DicReader = DictionaryReader()\n",
    "DicReader.tagger(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b537513-532c-4501-8687-30593624ac1c",
   "metadata": {},
   "source": [
    "Here we see a problem: The ambiguous word å¤§äººæ°— is marked as two tokens. Does bert use the same tokenizer? (It uses unidic-lite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966856f9-1cf4-4ae3-9d25-d0b857104943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512d0eff-6a06-4c22-8c44-3145f54ef543",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "print(input_ids)\n",
    "print([tokenizer._convert_id_to_token(input_id) for input_id in input_ids])\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085fa06-7c67-4801-96ac-9b3befd401d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ä¸€æ™‚\" in list(tokenizer.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7058518-0af6-4b28-ab94-7269f1f0683d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73c8ae-87f0-4b21-9fb3-4a2258c1382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab[\"ä¸€æ™‚\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b01c1b8-a60d-498f-b85e-908560f9ab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"ä¸€æ™‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135ed3b-896f-4121-a73f-fbd5467df197",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f583e5f7-6928-4dc6-b076-7a4327c7d05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([\"ä¸€æ™‚\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040c96c6-e1fb-4c56-9f92-0f61828080be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode([\"ä¸€æ™‚\"], add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda98944-2e08-4aee-8ff7-b33c88e4e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67b0a11-3371-4200-b64d-3ca86c4bada4",
   "metadata": {},
   "source": [
    "Note this is not a contextual embedding yet, let's look at it after contextualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871ce492-b9ae-4e74-aedf-c474d4a67a27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"cl-tohoku/bert-base-japanese-v2\")\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc19e91-4ee2-4e05-aa9e-1b3169ffa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    # max_length=4,\n",
    "    # truncation=True,\n",
    "    # padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ")  # needs to be pytorch tensors\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "\n",
    "print(input_ids.shape)\n",
    "\n",
    "outputs = model.forward(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "print(outputs.last_hidden_state)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c1840-a4d1-4cf7-9068-c7e109e01826",
   "metadata": {},
   "source": [
    "Now let's add a word to the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59f3bcc-bfaa-4601-a095-47ec1ecb70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_tokens([\"å¤§äººæ°—\"])\n",
    "model.resize_token_embeddings(\n",
    "    len(tokenizer)\n",
    ")  # Resize the dictionary size of the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d69fd5-addb-4997-b1c8-93d9e44c2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ec157e-b923-41fa-b5b2-c6fca1a2791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    add_special_tokens=False,\n",
    ")\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "print(input_ids)\n",
    "print([tokenizer._convert_id_to_token(input_id) for input_id in input_ids])\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d11ebf0-a5cd-407a-b509-1035a15cffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoded = tokenizer(\n",
    "    text,\n",
    "    # max_length=4,\n",
    "    # truncation=True,\n",
    "    # padding=\"max_length\",\n",
    "    return_tensors=\"pt\",\n",
    "    add_special_tokens=False,\n",
    ")  # needs to be pytorch tensors\n",
    "input_ids = text_encoded[\"input_ids\"]\n",
    "input_mask = text_encoded[\"attention_mask\"]\n",
    "\n",
    "print(input_ids.shape)\n",
    "\n",
    "outputs = model.forward(input_ids=input_ids, attention_mask=input_mask)\n",
    "\n",
    "print(outputs.last_hidden_state)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
